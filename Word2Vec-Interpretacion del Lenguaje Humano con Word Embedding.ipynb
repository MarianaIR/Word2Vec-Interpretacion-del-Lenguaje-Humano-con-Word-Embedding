{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ahcamachod/1893-word2vec-interpretacion-del-lenguaje-humano-con-word-embedding/blob/main/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"e7hqJQuyCDd0"},"source":["#<font color=red>**ONE-HOT-ENCODING**</font>\n","\n","---\n","\n","\n","\n","En este notebook desarrollaremos un clasificador de noticias para una start-up de noticias llamada **Alura Latam News**.\n","\n","El modelo Word2Vec utilizado para entrenar nuestro clasificador fue tomado de la siguiente fuente:\n","\n","Aitor Almeida, Aritz Bilbao Jayo. (2018) \"Word2vec models for the Spanish Language.\" Available from: https://github.com/aitoralmeida/spanish_word2vec, NO funciono!, en lugar utilize **SBW-vectors-300-min5.bin.gz**"]},{"cell_type":"markdown","metadata":{"id":"yHDdHhYiDDhn"},"source":["###**EXPLORANDO EL DATASET**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":6234,"status":"ok","timestamp":1720552277523,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"UUMMYA_kBtPe","outputId":"3e7dd40c-994b-46ca-c9f0-73e278960c3b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     fecha                                             titulo  \\\n","58802  2022-04-03 21:26:27  En una programación atípica, se disputó la ter...   \n","31234  2022-03-29 04:06:00  Salud y pensiones serán resueltas por la ciuda...   \n","66467  2022-04-02 06:00:00                               La sociedad mexicana   \n","61864  2022-03-31 09:16:54  Georgia.- El presidente de Osetia del Sur dice...   \n","81080  2022-03-22 00:00:00  El Gobierno acordó retrotraer precios de los a...   \n","\n","      pais                                           extracto  \\\n","58802   AR                                       FÚTBOL LOCAL   \n","31234   DO  En República Dominicana no se garantiza el der...   \n","66467   MX  El fiscal incómodo. El sainete dramático que p...   \n","61864   ES  El presidente de la región separatista georgia...   \n","81080   AR  La Secretaría de Comercio Interior que encabez...   \n","\n","                                                 resumen  \\\n","58802  (Foto: Bernardo Rolón / prensa Cadetes)\\n En u...   \n","31234  Millizen Uribe En República Dominicana no se g...   \n","66467  El fiscal incómodo. El sainete dramático que p...   \n","61864  Destaca que podría unirse a Osetia del Norte y...   \n","81080  El Gobierno afirmó que acordó con supermercado...   \n","\n","                                                  enlace             categoria  \n","58802  https://quedigital.com.ar/deportes/en-una-prog...              deportes  \n","31234  https://hoy.com.do/salud-y-pensiones-seran-res...  ciencia y tecnologia  \n","66467  https://www.jornada.com.mx/notas/2022/04/02/ec...              economia  \n","61864  https://www.notimerica.com/politica/noticia-ge...              politica  \n","81080  https://sinmordaza.com/noticia/265531-el-gobie...              economia  "],"text/html":["\n","  <div id=\"df-8ce55031-2bd9-47cd-836c-8861ccfc950a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fecha</th>\n","      <th>titulo</th>\n","      <th>pais</th>\n","      <th>extracto</th>\n","      <th>resumen</th>\n","      <th>enlace</th>\n","      <th>categoria</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>58802</th>\n","      <td>2022-04-03 21:26:27</td>\n","      <td>En una programación atípica, se disputó la ter...</td>\n","      <td>AR</td>\n","      <td>FÚTBOL LOCAL</td>\n","      <td>(Foto: Bernardo Rolón / prensa Cadetes)\\n En u...</td>\n","      <td>https://quedigital.com.ar/deportes/en-una-prog...</td>\n","      <td>deportes</td>\n","    </tr>\n","    <tr>\n","      <th>31234</th>\n","      <td>2022-03-29 04:06:00</td>\n","      <td>Salud y pensiones serán resueltas por la ciuda...</td>\n","      <td>DO</td>\n","      <td>En República Dominicana no se garantiza el der...</td>\n","      <td>Millizen Uribe En República Dominicana no se g...</td>\n","      <td>https://hoy.com.do/salud-y-pensiones-seran-res...</td>\n","      <td>ciencia y tecnologia</td>\n","    </tr>\n","    <tr>\n","      <th>66467</th>\n","      <td>2022-04-02 06:00:00</td>\n","      <td>La sociedad mexicana</td>\n","      <td>MX</td>\n","      <td>El fiscal incómodo. El sainete dramático que p...</td>\n","      <td>El fiscal incómodo. El sainete dramático que p...</td>\n","      <td>https://www.jornada.com.mx/notas/2022/04/02/ec...</td>\n","      <td>economia</td>\n","    </tr>\n","    <tr>\n","      <th>61864</th>\n","      <td>2022-03-31 09:16:54</td>\n","      <td>Georgia.- El presidente de Osetia del Sur dice...</td>\n","      <td>ES</td>\n","      <td>El presidente de la región separatista georgia...</td>\n","      <td>Destaca que podría unirse a Osetia del Norte y...</td>\n","      <td>https://www.notimerica.com/politica/noticia-ge...</td>\n","      <td>politica</td>\n","    </tr>\n","    <tr>\n","      <th>81080</th>\n","      <td>2022-03-22 00:00:00</td>\n","      <td>El Gobierno acordó retrotraer precios de los a...</td>\n","      <td>AR</td>\n","      <td>La Secretaría de Comercio Interior que encabez...</td>\n","      <td>El Gobierno afirmó que acordó con supermercado...</td>\n","      <td>https://sinmordaza.com/noticia/265531-el-gobie...</td>\n","      <td>economia</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ce55031-2bd9-47cd-836c-8861ccfc950a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8ce55031-2bd9-47cd-836c-8861ccfc950a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8ce55031-2bd9-47cd-836c-8861ccfc950a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2dba0c0a-370d-4025-82a6-8c3efc620cb8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2dba0c0a-370d-4025-82a6-8c3efc620cb8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2dba0c0a-370d-4025-82a6-8c3efc620cb8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","repr_error":"0"}},"metadata":{},"execution_count":44}],"source":["# 1°) Importamos PANDAS, cargamos los archivos de Entrenamiento y Prueba que ya nos dio el profesor del aula(excel)--->\n","\n","import pandas as pd\n","\n","entrenamiento = pd.read_csv(\"/content/noticias_entrenamiento.csv\")\n","test = pd.read_csv(\"/content/noticias_prueba.csv\")\n","\n","# Vemos las primeras 5 filas de los dataset.\n","entrenamiento.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1720552277524,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"EAHSWGgODhB1","outputId":"ae8e0b93-57b6-46e1-a05e-a32282e15538"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     fecha                                             titulo  \\\n","21662  2022-04-02 08:00:00  Subclases de proteínas, claves en patologías l...   \n","13542  2022-03-18 16:00:00  Alesi Díaz estrena canción de Farruko a ritmo ...   \n","20832  2022-03-23 06:57:59  Nueva caída en el precio de la luz: estas son ...   \n","2739   2022-03-30 00:00:00  IPC: La inflación sigue desbocada y sube un 9,...   \n","16013  2022-03-23 19:15:48  Muere la ex secretaria de Estado estadounidens...   \n","\n","      pais                                           extracto  \\\n","21662   ES  Guadalupe Sabio, del CNIC, estudia la activaci...   \n","13542   DO  El exponente de merengue urbano, Alesi Diaz, p...   \n","20832   ES  El precio medio será de 212,44 euros el megava...   \n","2739    US  Los augurios se han cumplido y la guerra ha pr...   \n","16013   PA  La exsecretaria de Estado estadounidense Madel...   \n","\n","                                                 resumen  \\\n","21662  Regístrate gratis en Diario Médico. Para segui...   \n","13542  Entretenimiento viernes, 18 de marzo de 2022 S...   \n","20832  En el mes de marzo el precio de la luz no ha p...   \n","2739                                                 NaN   \n","16013  Asistente Financiero\\n\\nasistentefinanciero.co...   \n","\n","                                                  enlace             categoria  \n","21662  https://www.diariomedico.com/medicina/endocrin...  ciencia y tecnologia  \n","13542  https://listindiario.com/entretenimiento/2022/...       entretenimiento  \n","20832  https://www.larazon.es/economia/20220323/vs4nl...              economia  \n","2739         https://www.youtube.com/watch?v=PpHOsXZAgFQ              economia  \n","16013  https://www.prensa.com/mundo/muere-la-ex-secre...                 mundo  "],"text/html":["\n","  <div id=\"df-feeb3cc1-e83e-4294-a075-72051787b104\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fecha</th>\n","      <th>titulo</th>\n","      <th>pais</th>\n","      <th>extracto</th>\n","      <th>resumen</th>\n","      <th>enlace</th>\n","      <th>categoria</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>21662</th>\n","      <td>2022-04-02 08:00:00</td>\n","      <td>Subclases de proteínas, claves en patologías l...</td>\n","      <td>ES</td>\n","      <td>Guadalupe Sabio, del CNIC, estudia la activaci...</td>\n","      <td>Regístrate gratis en Diario Médico. Para segui...</td>\n","      <td>https://www.diariomedico.com/medicina/endocrin...</td>\n","      <td>ciencia y tecnologia</td>\n","    </tr>\n","    <tr>\n","      <th>13542</th>\n","      <td>2022-03-18 16:00:00</td>\n","      <td>Alesi Díaz estrena canción de Farruko a ritmo ...</td>\n","      <td>DO</td>\n","      <td>El exponente de merengue urbano, Alesi Diaz, p...</td>\n","      <td>Entretenimiento viernes, 18 de marzo de 2022 S...</td>\n","      <td>https://listindiario.com/entretenimiento/2022/...</td>\n","      <td>entretenimiento</td>\n","    </tr>\n","    <tr>\n","      <th>20832</th>\n","      <td>2022-03-23 06:57:59</td>\n","      <td>Nueva caída en el precio de la luz: estas son ...</td>\n","      <td>ES</td>\n","      <td>El precio medio será de 212,44 euros el megava...</td>\n","      <td>En el mes de marzo el precio de la luz no ha p...</td>\n","      <td>https://www.larazon.es/economia/20220323/vs4nl...</td>\n","      <td>economia</td>\n","    </tr>\n","    <tr>\n","      <th>2739</th>\n","      <td>2022-03-30 00:00:00</td>\n","      <td>IPC: La inflación sigue desbocada y sube un 9,...</td>\n","      <td>US</td>\n","      <td>Los augurios se han cumplido y la guerra ha pr...</td>\n","      <td>NaN</td>\n","      <td>https://www.youtube.com/watch?v=PpHOsXZAgFQ</td>\n","      <td>economia</td>\n","    </tr>\n","    <tr>\n","      <th>16013</th>\n","      <td>2022-03-23 19:15:48</td>\n","      <td>Muere la ex secretaria de Estado estadounidens...</td>\n","      <td>PA</td>\n","      <td>La exsecretaria de Estado estadounidense Madel...</td>\n","      <td>Asistente Financiero\\n\\nasistentefinanciero.co...</td>\n","      <td>https://www.prensa.com/mundo/muere-la-ex-secre...</td>\n","      <td>mundo</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-feeb3cc1-e83e-4294-a075-72051787b104')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-feeb3cc1-e83e-4294-a075-72051787b104 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-feeb3cc1-e83e-4294-a075-72051787b104');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-457638c3-4fe7-4718-bf76-83e54b112a1b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-457638c3-4fe7-4718-bf76-83e54b112a1b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-457638c3-4fe7-4718-bf76-83e54b112a1b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","repr_error":"0"}},"metadata":{},"execution_count":45}],"source":["# Vemos las primeras 5 filas de los dataset.\n","test.sample(5)\n","\n","# Nos devuelve los dataset con varias filas y columnas."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1720552277524,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"CMhvGE-QDhEZ","outputId":"064c9871-f68c-450e-cea1-ca1bb30e4e3a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(91844, 7)"]},"metadata":{},"execution_count":46}],"source":["# 2°) Vemos los tamaños de los dataset --->\n","\n","entrenamiento.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1720552277524,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"xR_CaJf--trl","outputId":"851aba8f-5783-424b-ad32-81a0b1349f8c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(22961, 7)"]},"metadata":{},"execution_count":47}],"source":["test.shape\n","\n","# Los 2 dataset nos da un total de 113.000 noticias para entrenar y validar el modelo."]},{"cell_type":"markdown","metadata":{"id":"pbbzr1fEAXcj"},"source":["###**ONE-HOT-ENCODING**\n","\n","Vamos a hablar específicamente sobre el vector de palabras. Entonces, hemos hablado de vector de textos, vector de palabras, que es lo que queremos. Entonces la idea es reproducir nuestras palabras utilizando técnicas de incrustación de palabras también conocidas en inglés, como Word Embedding.\n","Entonces para poder entender en qué consiste la <font color=orage>Vectorización de palabras</font>, vamos a hablar un poco sobre cómo funciona el aprendizaje de idiomas. Digamos cuando tú estás estudiando otra lengua, por ejemplo el inglés, entonces, quieres aprender a decir, a hacer la pregunta, por ejemplo: ¿usted tiene carro? Entonces, uno aprende que la traducción es: Do you have a car?\n","Entonces tú comienzas a practicar Do you have a car? Do you have a car? Do you have a car? Do you have a car? Hasta que llega un punto que ya de tanto usar y de tanto ver la misma frase como que se te queda grabada y ya automáticamente, ya sabes cómo preguntar: ¿usted tiene un carro? en inglés, Do you have a car?\n","Ahora, ¿qué sucede después de haber utilizado tanto esa frase? Supongamos que quieres aprender una nueva frase. Entonces, en vez de preguntar, ¿usted tiene carro? Ahora vas a preguntar, será que ¿ella tiene carro? Entonces, la traducción automáticamente en inglés sería: Does she have a car? Entonces Does she have a car? Does she have a car? Does she have a car?\n","\n","***Y esa repetición entonces ya empieza a mostrar una especie de patrón***, una semejanza, algo que se torna repetitivo que será entonces lo único que cambia es la parte inicial de la frase o algunas palabras de ella, pero el resto se mantiene igual. Para el caso. ¿Usted tiene un carro? Sería: Do you have a car? Have a car permanece intacto, solo la primera parte de la frase es la que cambia.\n","En el siguiente caso, ¿ella tiene carro? Does she have a car? Entonces noten ustedes que have a car se mantiene igual y lo único que cambiaría serían estas frases o estas palabras iniciales de la pregunta. Pero ahora Does she have a car? Does she have a car? Does she have a car? ¿Y qué sucede si en algún punto, ella tiene bicicleta?\n","\n","Después de haber practicado tanto, ya como que nuestro cerebro de alguna forma entiende que ya la pregunta no va a ser: Does she have a car? Does she have a car? Sino Does she have a bike? Entonces se mantiene toda esta primera parte de la pregunta, Does she have a? Y lo único que cambia sería este sustantivo en vez de carro sería bicicleta. Bike.\n","Entonces así es como nosotros, los seres humanos, vamos también aprendiendo los idiomas. Todo con la repetición y con el uso y con la práctica. ¿Entonces, cómo funciona la estructura de un vector de palabras, digamos?\n","Nosotros al entender, al lograr comprender la estructura de una frase, lo único que debemos hacer es básicamente intercambiar las palabras porque ya la estructura ya quedó como que de una cierta forma almacenada pues en nuestra memoria, en nuestro cerebro, y por eso podemos elaborar nuevas frases.\n","Entonces partimos de una frase y a partir de esta frase original podemos crear nuevas frases, varias frases, intercambiando las palabras. Entonces, de esa manera nosotros volvemos nuestro conocimiento más general y al combinar palabras, entonces se obtienen nuevas frases.\n","\n","Y ahí es donde entramos entonces en el área del <font color=orage>Word Embedding</font>. ***Entonces vectorizar palabras no es otra cosa más que una representación numérica de la palabra. Entonces es simplemente yo tomar una palabra y representarla a través de números en un vector que sería un array de una dimensión y puede tener diversas posiciones.***\n","\n","Puedo decir que yo represento esa palabra con, digamos, dos números. 3 50, 100, 200, 300, 500, 1000, los números que yo quiera, siempre y cuando los mantenga en una dimensión, entonces por eso se llama vector. Es un vector, la palabra representada, digamos con un número que está dividido o varios números que se encuentran en un vector.\n","Entonces, retomando aquí tenemos la visualización de las palabras, representación numérica de palabras y para ello vamos a emplear el siguiente **ejemplo**:\n","\n","Tenga un buen día.\n","\n","Tenga un excelente día.\n","\n","Tenga un pésimo día.\n","\n","Tenemos estas tres frases, estas tres frases van a ser como nuestra base de datos, nuestra base de datos que contiene muchísimas frases, muchísimas palabras, billones de expresiones.\n","Digamos, imagínate, que tomamos toda Wikipedia en español, tomamos todos los libros que se encuentran en español, tomamos noticias, tomamos reportajes, artículos, libros. Bueno, de todo el contenido que pueda ser posible para poder entonces tener muchísimo, digamos muchísimo, muchísimo contenido.\n","\n","Entonces esto es lo que se conoce como el ***Corpus***. Por eso es que entrenar un modelo para generar vectores de palabras requiere de muchísimos, muchísimos datos, billones de palabras. Entonces ese es nuestro corpus. Ahora bien, si yo quiero vectorizar estas palabras, entonces lo primero que debo hacer es analizar ese corpus y encontrar las palabras de forma individual, que no se repita.\n","\n","En estas tres frases tengo tres estructuras, pero hay algo que me las diferencia, que sería lo siguiente:\n","\n","Tenga un buen día, tengo esa frase y tengo cuatro palabras distintas en la siguiente frase. Solo hay una palabra nueva que yo estoy añadiendo que sería excelente y la siguiente frase sería pésimo.\n","Entonces estas palabras que no se repiten, se constituyen como mi vocabulario y el <font color=orage>vocabulario entonces va a ser: “tenga un buen, excelente, pésimo día” son las palabras individuales y son estas palabras a las cuales yo debo asignar un valor numérico a cada una de ellas. ¿Entonces de qué forma?\n","\n","Simplemente le colocó un número, entonces tenga va a ser la número 1, un va a ser la número 2, buen va a ser la número 3, excelente va a ser la número 4, pésimo va a ser la número 5 y día será la número 6, entonces ya tenemos nuestras palabras y cada una tiene un número asociado a ellas.\n","\n","***¿Qué mecanismo podríamos usar o de qué manera yo puedo entonces emplear esta información disponible para poder asignar o dar una representación numérica a mis palabras?” Entonces, ahí es cuando llega una alternativa, que sería One-Hot-Encoding.***\n","Simplemente consiste en tomar yo cada una de estas palabras y asignarles un valor a cada una de ellas. Y, por ejemplo, supongamos aquí el 1 es tenga, el 2 es un, el 3 es buen, el 4 es excelente, el 5 es pésimo y el 6 es día. ¿De qué forma yo puedo entonces asignar valores aquí sin que se vayan a repetir?\n","\n","La forma más sencilla es yo poner cualquier valor aquí dentro, pero que no se me repita dentro de esta tabla. ¿Qué puedo hacer yo? Entonces pensemos un instante. One-Hot-Encoding. Uno solo, una una codificación, digamos, con un caliente, sí. Entonces, ¿qué podríamos hacer? Simplemente asignarle 1.\n","\n","Va a ser 1 0 0 0 0 0, ese va a ser el vector que representa tenga. Es un vector de 1 x 6, 6 espacios. Un. Entonces, la representación de un va a ser 0 1 0 0 0 0. Buen va a ser 0 0 1 0 0 0. Excelente va a ser 0 0 0 1 0 0. Pésimo va a ser 0 0 0 0 1 0. Y día va a ser 0 0 0 0 0 1. De esa manera, yo estoy garantizando que cada una de mis palabras va a tener un vector diferente.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U0YUZFvCIugT"},"source":["**Definicion:**\n","\n","***Analizando One-Hot-Encoding***\n","\n","Vamos a estudiar algunas formas de representar una palabra como un vector. Una de las representaciones estudiadas se conoce como One-Hot Encoding, que transforma palabras en un vector binario.\n","\n","***Considerando las frases anteriores, ¿Cuántas dimensiones tendrán los vectores de cada palabra?***\n","\n","* 6 dimensiones.Tenemos seis palabras diferentes en nuestro “corpus”; de este\n","  modo, cada palabra se convierte en una dimensión del vector."]},{"cell_type":"markdown","metadata":{"id":"myh5Q5SpJQ8V"},"source":["###**UTILIZANDO COUNT VECTORIZER**\n","\n","Vamos a utilizar ONE-HOT-ENCODING utilizando Python."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRagYZ4hAeI_"},"outputs":[],"source":["# 1°) Importamos el paquete CountVectorizer de la biblioteca Sklearn --->\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Instanciamos:\n","vector = CountVectorizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1HqYiAKAeL7"},"outputs":[],"source":["# 2°) Creamos Variable TEXTO y vamos a colocar una Lista con las frases que vimos en nuestro ejemplo --->\n","\n","# Corpus:\n","texto =[\n","    \"tenga un buen dia\",\n","    \"tenga un excelente dia\",\n","    \"tenga un pesimo dia\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1720552277527,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"VW-33GacAeXw","outputId":"c97724b6-e728-4f5b-cb83-08d10fcee22e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer()"],"text/html":["<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":50}],"source":["# 3°) Con el TEXTO vamos a hacerle un FIT a nuestro Vector --->\n","\n","vector.fit(texto)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1720552277527,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"UfZdJYxda71-","outputId":"de1dbb6b-647d-4598-b061-6fbfd67006bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tenga': 4, 'un': 5, 'buen': 0, 'dia': 1, 'excelente': 2, 'pesimo': 3}"]},"metadata":{},"execution_count":51}],"source":["# 4°) Si tomamos el Vector ya podemos ver el Vocabulario --->\n","\n","vector.vocabulary_\n","\n","# Nos devuelve un Diccionario con valores asignados a cada palabra."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1720552277527,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"m1k3iJhNa74m","outputId":"94c21f97-61b8-438b-9cff-697789f1ca2a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<1x6 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 1 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":52}],"source":["# 5°) Si queremos saber como quedo el Vector para la palabra BUEN por ejemplo --->\n","\n","vector_buen = vector.transform([\"buen\"])\n","vector_buen\n","# Vemos que el VECTOR_BUEN nos dice que es una Matriz Sparse"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1720552277528,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"fYLKn-mia77h","outputId":"e96bd5ff-8951-4f7f-ebde-b203fd3b4f97"},"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 0)\t1\n"]}],"source":["print(vector_buen)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1720552277528,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"kcXNvms9cGLK","outputId":"e0ad25f7-7bcb-460c-8f32-6778a4833dc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 0 0 0 0 0]]\n"]}],"source":["# 6°) Hacemos un print de vector_buen y lo convertimos en un ARRAY --->\n","\n","print(vector_buen.toarray())\n","\n","# Nos devuelve la cantidad de palabras del Corpus que NO se repiten con sus posiciones, las 6 posiciones de cada palabra."]},{"cell_type":"markdown","metadata":{"id":"F0z7EQdWcxdW"},"source":["Si agregamos mas palabras al CORPUS el Vocabulario va a aumentar y si seleccionamos otra palabra agregada como Vector en el ARRAY nos va a mostrar la posicion.\n","\n","Cuando tenemos un CORPUS de 50.000 palabras por ejemplo, nos devolveria 49.000 ceros, y usar one-hot-encoding NO seria lo adecuado usar cuando son muchas palabras, para esto se utilizan otras alternativas para lograr un mejor uso de nuestros Vectores."]},{"cell_type":"markdown","metadata":{"id":"MztC9UCvd8CR"},"source":["#<font color=red>**WORD2VEC PRIMER CONTACTO**</font>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"0LPNMAQ7eCXf"},"source":["###**CONOCIENDO WORD2VEC**\n","\n","***¿Será que existe alguna técnica que nos permita representar las palabras con un vector de un tamaño fijo que no cambie y aun así, pues tener la posibilidad de evitar que por causa del tamaño de nuestro vocabulario la única alternativa sea One-Hot-Encoding?***\n","\n","Recordemos que si tengo 50.000 palabras, entonces voy a tener 49999 ceros y solamente un 1. ¿Entonces, cuál será la posibilidad que tenemos? Encontramos entonces, dentro de las muchas técnicas una que se llama <font color=orage>Word2Vec</font>. Entonces vamos representando palabras de acuerdo con el contexto de estas palabras.\n","\n","Entonces veamos detenidamente en qué consiste Word2Vec. Supongamos que yo tengo el siguiente grupo de palabras: tengo ***Francia, Rusia, Italia, España, Peugeot, Ferrari***. Tengo esas palabras. Entonces al yo tener esas palabras, y tratar de pensar en una representación podríamos tal vez llegar a la conclusión que una alternativa que tenemos sería la de utilizar un ***plano cartesiano.***\n","\n","Recordemos que el ***plano cartesiano*** es una representación en dos dimensiones de una posición, una determinada posición. Entonces tenemos el eje de las X y el eje de las Y. Entonces, pensemos nuevamente en nuestro vocabulario o en las palabras que tenemos Francia, Rusia, Italia, Ferrari, España, Peugeot. ¿De qué manera yo podría colocar estas palabras dentro de mi plano cartesiano?\n","\n","Entonces tenemos nuestras palabras nuevamente Francia, Peugeot, España, Rusia, Ferrari, Italia. ¿Cómo yo ubicaría estas palabras en el plano cartesiano? Entonces yo tomaría mi plano cartesiano, el eje de las X y de las Y y colocaría, por ejemplo, Francia en esa posición donde ustedes la ven, Italia, España y un poquito más para la parte superior, Rusia.\n","Y después, más como para el lado derecho, Peugeot y Ferrari. ***¿Por qué hice eso?*** Nosotros o en mi caso, de tanto escuchar noticias, de haber ido al colegio, de haber ido a la Universidad, de estar continuamente digamos actualizándome con los acontecimientos del mundo, pues sé que por ejemplo, España, Italia y Francia se encuentran en el continente europeo.\n","\n","Entonces yo hago como esa analogía en mi cabeza, y trato de colocar estas estos nombres de países, sé que son nombres de países como en un lugar. Y como Rusia sé que pertenece a otro continente, lo alejo un poquito, digamos lo primero que ello observé al ver ese vocabulario fue que tenía cuatro países y tenía dos marcas de carros.\n","Entonces por un lado, pues separé mis países y los coloqué en un lugar. Y por otro lado, también coloque las marcas de carro Ferrari y Peugeot en otro lugar. Pero también noten ustedes, yo sé que Peugeot, también de tanto, pues a mí me gustan los carros y de conocer de carros yo sé que Peugeot es francés y sé que la marca Ferrari es italiana.\n","\n","Entonces de cierta forma yo también los coloqué en una posición que para mí, que a mi parecer tiene sentido. Entonces esto es lo que me permitió a mí decir, mira, esta es la mejor forma para mí, a mi concepto, de acuerdo con mi forma de ver, cómo quedarían localizadas estas palabras en un plano cartesiano.\n","\n","<font color=orage>Cuando yo hago esto, entonces estoy utilizando los principios de Word2Vec</font>. Y cuando yo digamos, en vez de utilizar el plano cartesiano tengo las palabras de forma aleatoria en un espacio cualquiera, en un espacio multidimensional, no pensemos ya más en dos dimensiones, sino pensemos, por ejemplo, aquí en este caso en 6 dimensiones, entonces estaríamos hablando de One-Hot-Encoding.\n","\n","Contemos bien, 1, 2, 3, 4, 5, 6. ¿Por qué dije 6 dimensiones? Porque tengo 6 palabras y recordemos que para poder representar cada palabra, entonces yo utilizaría un 1, y el resto ceros. Entonces esa sería una forma de yo hablar también de One-Hot-Encoding.\n","\n","Entonces básicamente, así es como funciona Word2Vec."]},{"cell_type":"markdown","metadata":{"id":"Y32wMrGkMWzs"},"source":["###**APRENDIZAJE HUMANO**\n","\n","Pudimos generar una representación vectorial, con una dimensión de 1 por 2, con dos espacios para localizar las palabras de nuestro vocabulario, los países y las marcas de carros.\n","\n","***¿Entonces de qué forma es que Word2Vec hace estos procesos? Vamos a ver con otro ejemplo.***\n","\n","Entonces, supongamos que yo tengo la siguiente frase: **Vivo en_________, y al final tengo un país lindo**. Entonces al yo ver esta frase en mi cabeza, asocio que esa palabra que va en blanco, en este espacio en blanco, debe hacer referencia a algún país, entonces yo podría decir, vivo en Rusia, un país lindo, vivo en Francia, un país lindo, vivo en Italia, un país lindo, vivo en España, un país lindo.\n","\n","O también, por ejemplo, otra frase: conduzco un, tengo un espacio en blanco y rojo, entonces yo ya sé el verbo conducir. Entonces, ¿qué puedo conducir? Puedo conducir un vehículo. Eso es lo primero que viene a mi cabeza entonces podría decir que es un Ferrari, que es un Peugeot, que es un Ford o cualquier marca de carro que ustedes se les pase por su cabeza en ese momento.\n","Entonces, ¿nosotros cómo llegamos a estas conclusiones? Vuelvo a repetir, como ya tenemos un recorrido por la vida, viendo tanto contenido, viendo televisión, leyendo, estudiando geografía, viendo documentales, películas, y conversando con la gente, entonces, todo este contenido va permitiendo que nuestro cerebro realice estas asociaciones y podamos entonces, a través de un contexto de palabras, poder definir cuál es la palabra que mejor se encaja en una frase.\n","Entonces, aprendemos nosotros, los seres humanos aprendemos consumiendo contenido, mucho contenido, esa es la forma de aprendernos, así nos culturizamos.\n","\n","***Entonces, ¿cómo enseñarle el contexto a la máquina? ¿Será que así como nosotros, los seres humanos aprendemos, será que los computadores también pueden aprender?***\n","\n","Entonces ahí entramos en el área de **NLP o Natural Language Processing o procesamiento del lenguaje humano**, y aquí es donde ya se empiezan a desarrollar todas estas técnicas que lo que hacen es entrenar nuestros Word Embeds, que serían como estos vectores densos para que el computador pueda entonces contextualizar las palabras.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OkPC_b7sTl_-"},"source":["###**ARQUITECTURAS WORD2VEC**\n","Vamos a hablar un poco sobre la arquitectura de Word2Vec. Entonces, si nosotros queremos que nuestra máquina aprenda digamos de la forma como tal vez nosotros los seres humanos aprendemos, lo primero que vamos a necesitar son datos, muchos y muchos datos, esto vendría siendo el corpus de nuestro Word Embedding.\n","Entonces necesitamos billones de palabras, muchas, muchas palabras tomadas de diversas fuentes, y mientras más grande el volumen de nuestros datos, también nuestros vectores van a poder generalizar mejor. Vamos a tener una representación vectorial de las técnicas de Word Embedding más precisa.\n","Vamos a tener digamos un procesamiento de lenguaje natural mucho más cercano a la realidad. Es como nosotros. Mientras más contenido consumimos, mientras más leemos, mientras más nos culturizamos, pues más fácil vamos a poder hablar de diversos temas.\n","Entonces lo mismo lo hace el computador, entonces lo primero, muchos datos, billones y billones de datos, de palabras, perdón, billones y billones de palabras, mucho contenido. Y todas esas palabras las vamos a alimentar a los algoritmos de Word2Vec.\n","\n","El algoritmo de Word2Vec es el que se encarga de generar estos vectores. Estos vectores pueden ser de diversos tamaños, como les dije, puede ser de 10, 50, 100, 300, 500, 1000 posiciones, es algo que nosotros mismos somos los que vamos a definirlo, para finalmente obtener entonces nuestro vector de eso que es representado por números, digamos tenemos nuestro vector.\n","\n","Un array de una dimensión por diversas posiciones, entonces, por ejemplo, tenemos aquí la palabra Ferrari, y tenemos esta representación vectorial de ella y aquí estos tres puntitos nos indica que puede tener el tamaño que nosotros vayamos a definir. Observar esa estructura una entrada digamos, algo que acontece en la mitad y después una salida, ¿qué nos recuerda?\n","\n","Bueno, para los que ya hicieron el curso de redes neuronales con NumPy, que también está aquí en nuestra plataforma de Alura Latam, entonces vemos aquí que se trata precisamente de una red neuronal. ¿Y cuál es la estructura de la red neuronal? ¿Cómo funciona la red neuronal?\n","\n","Vamos a abordarlo de una forma un poco general, para que más o menos entendamos cómo es que Word Embedding y, especialmente la técnica de Word2Vec, vamos a hablar de Word2Vec aquí específicamente nuestro curso funciona. Entonces en una red neuronal, nosotros tenemos una capa de entrada, una capa o capas escondidas ocultas y también tenemos una capa de salida.\n","\n","¿Cómo funciona la red neuronal? Tenemos nuestras capas. Entonces aquí yo tengo mis entradas, que vendría siendo muchos, muchos datos. Sería mi corpus, billones de palabras. Y cada una de estas palabras tiene un peso y un bies asociado a ellas. ¿Cuál peso y cuál bies?\n","Pues cuando el algoritmo recibe estas palabras por la primera vez, los va a asignar de forma aleatoria, porque así es como funciona la red neuronal, primero asigna pesos y bieses de forma aleatoria, y entonces va recorriendo toda la red, hasta llegar a la salida que vendría siendo el vector denso que contiene la representación numérica de cada una de esas palabras. ¿Pero qué sucede entonces?\n","\n","Yo tengo una suma de estos pesos y bieses y una función de activación que me permite avanzar por cada capa de mi red neuronal. Y al llegar a la salida entonces lógicamente si yo asigné pesos y bieses de forma aleatoria, lo más seguro es que voy a tener un error muy grande.\n","\n","Y este error es muy importante porque con este error yo voy a poder generar una función de costo que me va a permitir, digamos, devolverme por mi red y corregir estos pesos y bieses y hacer una serie de iteraciones aquí para de esa manera, entonces, acercarme mejor, tener una mejor aproximación a mi salida.\n","Entonces, más o menos de forma general, así es como funciona una red neuronal. Entonces, mientras más palabras, mientras más iteraciones, mientras más procesamiento se realice, entonces, mejor va a ser nuestro resultado, vamos a reducir como que toda esa entropía que es ese estado de desorden, de las palabras en general.\n","\n","Por eso para poder entrenar un modelo de Word Embedding se necesitan de billones de palabras y de mucho procesamiento. Entonces, de esta manera, nosotros ya vamos a comprender mejor cómo es que funciona Word2Vec. Veamos en principio cómo funcionaría CBOW, que sería Continuos Bag of Words, bolsa de palabras continuas sería una traducción literal, bag of words, continuous bag of words. Bueno.\n","\n","Entonces, supongamos: Vivo en un país lindo. ¿Será que para yo poder definir qué palabra va aquí, necesito todas estas palabras que concluyen, que me llevan a esta frase o será que si yo elimino algunas de esas palabras, aún podría obtener una respuesta aquí?\n","Entonces, veamos. Si yo quito por ejemplo, una preposición, un artículo indefinido y un adjetivo, que sería en un lindo, si yo quito eso, será que yo aún podría decir qué tipo de palabra llevo aquí. Yo creo que sí, humanamente yo sé que si yo tengo vivo y país lo más seguro es que ahí tenga un país, en ese espacio en blanco, lo que va es un país, porque vivir y país automáticamente me llevan a mí a pensar en un país.\n","\n","Retomando nuestra analogía de la red neuronal, entonces yo tengo capa de entrada, capa o capas ocultas y capa de salida. Entonces como capa de entrada yo voy a tener mi contexto que se va a hacer la palabra vivo y va a ser la palabra país, y la voy a llevar a mi capa oculta que sería dónde está mi Word2Vec, que aquí es donde aplico las funciones.\n","\n","Entonces digamos que va a ser Word2Vec, él me va a dar un diversas salidas, me va a decir que es Francia, que es Italia, que es España, en fin, él me va a dar varias salidas, entonces, cuando yo tomo el contexto para determinar la palabra como para establecer el vector denso que representa la palabra que yo necesito colocar en ese espacio en blanco, entonces estamos hablando de la técnica o del ***modelo CBOW, continuous bag of words.***\n","\n","Ahora bien, si yo no tomo el contexto, sino que tomo la palabra y quiero asumir el contexto, entonces por ejemplo yo tengo a los lados dos espacios en blanco y tengo en la mitad un país, lo más seguro es que yo voy a tratar de asociar otras palabras, trabajo en Francia, que queda en Europa. Vivo en Francia, que es un país bonito. Estudio en Francia.\n","Bueno, en fin, aquí pueden haber otras palabras que van a contextualizar ese término específico que yo tengo. Entonces, en este caso ya cambia el orden de mi red, ya ahora mi entrada va a ser una única palabra. Entonces voy a entrar a mi capa oculta, que es donde se va a encontrar mi Word2Vec y voy a tratar de obtener la salida que va a ser el contexto en el cual va esta palabra, que eso es lo que yo desconozco.\n","\n","Entonces, después de hacer todo este entrenamiento, yo al colocar mi entrada, llevarla a mi capa oculta y al obtener mi salida, yo después de varias iteraciones y de bastante aprendizaje, voy a determinar que el mejor contexto para Francia es vivo y país. Y esta otra técnica de Word2Vec es la que se conoce como ***Skip Gram.***\n","\n","Entonces no tomamos el contexto para asumir la palabra, sino que tomamos la palabra para asumir el contexto y en CBOW tomamos el contexto para descubrir la palabra. Esa es como la diferencia conceptual entre Skip Gram y Word2Vec. ¿Entonces, también en términos técnicos, que se podría decir?\n","\n","Que Skip Gram generaliza el mejor cuando, por ejemplo, tiene un corpus un poco más reducido. Entonces es más fácil en un algoritmo que se comporta mejor, pero CBOW por el contrario, es más rápido, es más ágil. Ahora bien, TensorFlow dice que no, que CBOW generaliza mejor.\n","\n","Entonces ya son diversos conceptos, pero la recomendación a nivel general es que se utilicen ambos acercamientos, tanto CBOW como Skip Gram, o sea se utilice tanto el contexto para descubrir la palabra como la palabra para descubrir el contexto, para de esa forma ver cuál modelo trabaja mejor."]},{"cell_type":"markdown","metadata":{"id":"cVgdCo1jUwwF"},"source":["###**BUSCANDO MODELOS WORD2VEC**\n","\n","Debemos cargar un modelo Word2Vec pre entrenado, que haya sido previamente entrenado.\n","Para poder entrenar un modelo de Word Embedding necesitamos un corpus con millones o con billones de palabras. Y mientras más palabras tengamos, entonces nuestros vectores van a tener una mayor precisión y eso es lo que queremos porque queremos que se acerque mucho a la realidad.\n","\n","Entonces, para poder hacer ese trabajo, bien sea a través de Deep Learning, bien sea a través de diversas técnicas de Word Embedding, lo importante es que el algoritmo realizado o la forma de realizar todos estos modelos sea fidedigna, sea buena, y para ello necesitamos un recurso confiable.\n","\n","Hay que buscar en Internet, digamos, hay que ver, por ejemplo Word Embedding en español. Y ustedes digitan, por ejemplo, en Google modelos de Word Embedding en español. Entonces hay diversos modelos, entonces ustedes pueden empezar a buscar y yo al realizar este curso estuve viendo varios modelos, pero realmente solo uno de ellos que encontré fue el que más me llamó la atención. Vamos a ver, aquí en Spanish a ver si lo encuentro.\n","Uno de ellos, y es este de aquí, de hecho, ya está subrayado. Vamos a darle clic. Entonces, es de estas personas de Aitor Almeida y de Aritz Bilbao. Aquí está la Lafuente. Ellos son Doctores en el tema de procesamiento del lenguaje natural, entonces es un modelo confiable, ya lo probé.\n","\n","Ustedes pueden inclusive buscar otros modelos, lo importante es que sean modelos de Word2Vec, de embedding, porque estamos en el curso de Word2Vec. Hay otras técnicas como fast text y otras.\n","Entonces aquí nos da una especificación: modelo son creados usando una ventana de más o menos 5 palabras, descartando aquellas con menos de 5 instancias y creando un vector de 400 dimensiones. Recordemos que nosotros habíamos hablado de que el tamaño del vector lo definíamos nosotros, 50, 100, 200, 300, bueno.\n","\n","El modelo pre entrenado que vamos a utilizar es de 400 dimensiones. Miren ustedes, el texto utilizado para crear los embeddings ha sido recuperado de noticias Wikipedia, de la BOE española, web scraping y recursos literarios abiertos o sea, muchos datos. El texto usado tiene un total de 3.257.000.000 palabras y 18.852.481.207 caracteres.\n","\n","3.200.000.000 de palabras y 18.000.000.000 de caracteres. Entonces es algo masivo, es algo enorme. Y bueno, pide que utilicemos la que citemos, pues si vamos a llegar a utilizar estos modelos, a Aitor Almeida, y Aritz Bilbao. Entonces vamos a ir al link externo que ellos nos están dando y nos lleva a una cuenta de GitHub.\n","\n","Nos dice que los modelos fueron compartidos en este link, de Zenodo, ahorita lo vamos a acceder para descargarlos y aquí también nos tiene una pequeña documentación de cómo cargarlo. Si queremos cargar el modelo completo, entonces vamos a utilizar Word2Vec.load pero también podemos utilizar KeyedVectors, que son vectores con una llave.\n","\n","Entonces pensemos en un diccionario, tenemos nuestras keys, que serían las palabras, y el vector que sería nuestro valor. Entonces, KeyedVectors para manejar ese concepto. Y vamos específicamente ya a trabajar con KeyedVectors y nuevamente hace la recomendación.\n","\n","Si usamos nuestros modelos, si usamos los modelos de ellos en nuestros programas, utilizamos la citación de ellos o la cita bibliográfica a la fuente de ese recurso. Entonces vamos a ir a Zenodo.org y aquí nos aparecen Spanish 3B words Word2Vec Embeddings. Entonces, aquí está el modelo completo, el vector completo, bueno, noten ustedes 7.4 gigas, gigante.\n","\n","Pero vamos a utilizar mejor, digamos el KeyedVectors que tiene 2.9 gigas. Entonces le vamos a dar clic a donde dice download al Keyed_vectors.zip, le dan download y él va a empezar a descargar. Yo, como ya lo descargué, vean, yo no voy a hacer esto porque ya lo descargué. Se va a demorar mucho. Entonces le voy a dar cancelar. Pero ustedes cargan el que dice Keyed_vectors.zip.\n","\n","Y al tener este Keyed_vectors.zip, entonces lo vamos a subir a nuestro Google Drive. Yo voy a venir a mis downloads, que yo ya lo tengo aquí y lo voy a subir a mi carpeta de Word2Vec. Entonces dice que se demora un poco de minutos, 20 minutos subiendo. Mientras él carga, vamos a estar aquí nuevamente viendo lo que dice estos modelos."]},{"cell_type":"markdown","metadata":{"id":"Ux5G9pCjluvK"},"source":["###**CARGANDO EL MODELO**\n","\n","Acabamos aquí entonces de cargar nuestros Keyed_vectors.zip a drive, pero si queremos conocer un poco mejor cómo es que lo vamos a implementar en Python, volvamos al GitHub de donde encontramos el link de descarga. Ellos trabajan con dos tipos de modelos, gensim full models que sería complete_model.zip, ese no fue el que descargamos y KeyedVectors.\n","Entonces ese es el archivo que acabamos de descargar, keyed_vectors.zip. Podemos ver las diferencias entre ellos en la siguiente URL, vamos a hacer clic en esta URL, vamos a abrirlo en una nueva guía. Y aquí dice almacenar y consultar word vectors.\n","\n","Dice aquí: este módulo implementa word vectors, vectores de palabras, y generalmente sets o conjuntos de vectores, keyed by lookup tokens, vectores que se encuentran con una llave a través de una token que vendría siendo el vector, vendría siendo el array de una dimensión con varias posiciones, direccionado hacia una key, que sería la palabra.\n","\n","Aquí dice que los trained word vectors, que los vectores de palabras entrenados son independientes de la forma como ellos fueron entrenados, Word2Vec, FastText, VarEmbed, diferentes, y ellos pueden ser representados por estructuras solas, como se implementa en este módulo.\n","Entonces, aquí hacen lo que explica es que la estructura llamada KeyedVectors, es esencialmente un mapeo entre llaves y vectores. Entonces cada vector es identificado por su llave de búsqueda y muy seguidamente, es una token que está en formato de string y esto usualmente se mapea entre sí.\n","Entonces tenemos la string y dice que nos lleva a un numpy array de una dimensión, o sea al vector. Y queda así como como una especie de diccionario. Entonces, aquí dice: ¿por qué utilizar KeyedVectors en vez de un modelo completo?\n","\n","Entonces dice que podemos continuar entrenando los vectores en KeyedVectors, dice que no tiene esta posibilidad, no podemos continuar entrenando los vectores. Para poder entrenar o actualizar vectores necesitamos el modelo completo pero todas las demás funcionalidades sí las poseen los KeyedVectors.\n","Entonces objetos más pequeños entonces dice que el KeyedVector sí lo soporta y el modelo completo no. Dice que los KeyedVectores son más pequeños y necesitan menos memoria RAM porque no necesitan almacenar el estado del modelo que habilita su entrenamiento. Almacenar y cargar del formato nativo de FastText y Word2Vec dice que KeyedVectors también lo hace.\n","\n","Entonces, los vectores que han sido exportados por Facebook y Google por las herramientas de Facebook y Google, no soportan más entrenamientos, pero los puede cargar a través de KeyedVectors. Entonces vemos que el KeyedVector es muy bueno. Podemos también hacer un append o añadir nuevos vectores.\n","\n","El KeyedVector lo permite, el modelo completo también. Concurrencia, entonces dice aquí que permite que las consultas concurrentes a vectores se realice, comparten RAM los dos y la forma de cargar dice que utiliza mmap para cargar los datos del disco de forma instantánea. Entonces por eso los KeyedVectors son muy interesantes.\n","\n","Aquí fue como un resumen. La principal diferencia es que los KeyedVectors no toleran o no soportan un entrenamiento más profundo y aquí, por ejemplo, dice cómo obtener los vectores. Entonces para poder entrenar un modelo completo podemos acceder a la propiedad modelo .wv que es el que mantiene el standalone keyed vectors. ¿Entonces, cómo se haría esto? Entonces se haría importando estos dos módulos e instanciándolos de esta forma.\n","\n","Ahora bien, si nosotros tenemos ya el keyed vector que se trata del archivo .kv que acabamos nosotros de descargar, entonces la forma de trabajarlo sería así. KeyedVector.load(‘vectors.kv’). Entonces habría que hacer esta importación y ejecutar, instanciar nuestro vector utilizando el comando .load del módulo KeyedVectors, de gensim models.\n","\n","Si venimos a nuestro notebook, entonces vamos a hacer lo siguiente: from gensim.models import KeyedVectors. Y ahora sí vamos a colocar aquí en nuestro modelo, entonces vamos a llamarlo simplemente modelo y ahí lo que vamos a hacer es KeyedVectors.load y vamos a cargar nuestro archivo kv, ¿pero qué sucede?\n","\n","Si venimos a nuestro Drive, vean ustedes, yo tengo en mi carpeta MyDrive, yo no tengo el archivo kv directamente sino yo tengo el archivo keyed_vectors.zip. Entonces, ¿qué voy a hacer? Voy a copiar el camino de este archivo y en una celda antes lo que voy a hacer es hacerle un unzip.\n","\n","Entonces a decir !unzip. Al yo colocar este signo de exclamación, lo que estoy haciendo es usar de una cierta forma la línea de comando de Linux, que funciona atrás de este Colab. Entonces le hacemos unzip. Y él se va a demorar algunos instantes, digamos descomprimiendo el archivo, el complete.kv y el complete.kv.vectors que sería un NumPy array, entonces él los mapea, mapea esos dos archivos.\n","\n","En el complete.kv se encuentran las keys y en el complete.kv.vectors.npy se encuentran sus respectivos tokens. Entonces, pues él se va a demorar algunos instantes y en cuanto pues se realiza este proceso, entonces nosotros aquí confirmamos que nos haya quedado bien escrito el código que es vector.log y por KeyedVectors. Básicamente esto. Entonces hay que dejar que él se ejecute.\n","\n","Después de que nuestro modelo esté ejecutado, entonces podemos tratar de ver qué operaciones o qué métodos le podemos aplicar para conocerlo un poco mejor."]},{"cell_type":"markdown","metadata":{"id":"wxzIxRRKhC3G"},"source":["#<font color=red>**EXPLORANDO WORD2VEC**</font>\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkgYz67MrI56"},"outputs":[],"source":["# 1°) Montamos el Drive al colab , que es donde esta el archivo Zip que descargamos del link --->\n","#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1720552277926,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"},"user_tz":180},"id":"K39sGieXrqjX","outputId":"c525a32b-604d-4adf-c4c6-80c3677df488"},"outputs":[{"output_type":"stream","name":"stdout","text":["4.3.2\n"]}],"source":["# 1°)\n","import gensim\n","print(gensim.__version__)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLWxKlL1vWyI"},"outputs":[],"source":["# 2°) Descomprimimos el zip--->\n","\n","#!unzip \"/content/drive/MyDrive/MachineLearningAvanzado/SBW-vectors-300-min5.bin.gz\"\n"]},{"cell_type":"code","source":["# 3°) Importamos biblioteca e Instanciamos el modelo --->\n","\n","from gensim.models import KeyedVectors\n","\n","# Ruta al archivo .bin.gz de embeddings\n","model = '/content/drive/MyDrive/MachineLearningAvanzado/SBW-vectors-300-min5.bin.gz'\n","\n","# Cargar el modelo Word2Vec desde el archivo .bin.gz\n","modelo = KeyedVectors.load_word2vec_format(model, binary=True)\n"],"metadata":{"id":"vzdKHV8YfQMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4°) Vamos a entender el modelo un poco mas, vamos a ver como vamos a trabajar con nuestros KeyedVectors --->\n","\n","#modelo.get_vector(\"rusia\")\n","vector = modelo['rusia'] # Use indexing to get the vector for 'rusia'\n","print(vector)"],"metadata":{"id":"D15mGYVdhHdP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552305463,"user_tz":180,"elapsed":767,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"b8360e1e-18dd-4fb6-a724-935e1cb770e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 2.41386495e-03 -2.75877677e-02  2.90296692e-02 -1.11172304e-01\n","  1.18620127e-01 -1.82610556e-01 -1.28264762e-02  3.63720506e-02\n","  3.33251119e-01 -1.22887135e-01  1.48669794e-01  8.25086087e-02\n"," -5.95312472e-03 -2.79966325e-01  5.39358258e-02 -1.29495278e-01\n","  1.06282616e-02 -1.46060437e-01 -4.77864712e-01  2.65246592e-02\n"," -2.99940377e-01  8.78467709e-02  1.29306212e-01  1.45441636e-01\n"," -9.87920910e-02 -4.43036593e-02 -3.25560659e-01  7.84606040e-02\n"," -1.71662077e-01  2.80851662e-01  1.36841699e-01  1.38896421e-01\n","  1.98732410e-02 -2.04700112e-01 -7.12859929e-02  1.29271090e-01\n","  2.09103078e-01  1.12479992e-01 -2.87536591e-01  9.74789113e-02\n"," -6.69664741e-02  1.32165611e-01  7.61287585e-02 -2.50587672e-01\n"," -3.48826014e-02  1.66718867e-02  1.02963755e-02  4.98696081e-02\n","  1.42625734e-01  1.06565237e-01  1.50579974e-01  1.86168533e-02\n"," -1.20499834e-01  8.89115408e-03  7.72292465e-02  9.14911628e-02\n"," -2.83374697e-01  1.41645759e-01  8.03587064e-02 -2.72485651e-02\n","  4.97856215e-02 -8.63490179e-02  8.54763612e-02 -2.12672830e-01\n","  2.87993494e-02  1.99069269e-02 -1.28568321e-01  7.07158074e-02\n"," -2.95887142e-01  2.94240922e-01 -2.34052569e-01  7.54365921e-02\n","  2.79225916e-01  2.37165898e-01  2.21507609e-01  8.42951089e-02\n"," -3.93167622e-02 -4.17294465e-02  4.86301519e-02 -7.96223432e-02\n","  5.81984073e-02 -2.90586501e-01 -2.33084243e-02 -3.26288417e-02\n","  5.96483797e-02  3.63400690e-02 -1.71641391e-02 -6.06889687e-02\n","  3.14325064e-01  1.04236424e-01 -1.92468971e-01 -8.89227912e-02\n","  1.17830373e-01  2.39545867e-01 -5.88892460e-01  1.37464032e-01\n","  5.09950668e-02  1.17628932e-01  2.50833124e-01  3.08994144e-01\n"," -1.11926347e-01 -3.05150568e-01 -1.07626520e-01 -9.88959372e-02\n"," -1.12209328e-01  2.01032713e-01  2.56908804e-01  1.30568389e-02\n"," -2.22845301e-01  2.74891376e-01  2.30436504e-01  4.04203124e-02\n","  1.34588644e-01  1.12795591e-01 -1.66850388e-01 -7.67121911e-02\n"," -1.33297905e-01  3.00552189e-01  1.67540312e-01 -2.35349879e-01\n","  1.45897642e-01 -5.33306114e-02 -5.63602485e-02 -2.10695505e-01\n","  8.83360133e-02 -5.79890870e-02 -2.77761351e-02 -5.18286169e-01\n","  2.41762847e-01  1.79799683e-02  4.05966712e-04  3.16708088e-02\n","  6.70051277e-02 -1.09437861e-01 -1.01744473e-01  1.11266226e-01\n"," -9.30901766e-02 -7.73900077e-02  1.38455346e-01 -7.92926401e-02\n"," -6.96376860e-02 -1.06211446e-01  7.45217130e-02 -2.65148371e-01\n"," -5.16676120e-02  3.36495750e-02  2.88876772e-01 -5.01837358e-02\n"," -6.08261004e-02 -4.48271222e-02  5.87273166e-02  1.49374694e-01\n","  5.76913133e-02 -3.80494654e-01 -1.74012691e-01 -1.09660558e-01\n"," -3.14781964e-02 -1.65736750e-02 -9.50594991e-02  4.92432788e-02\n","  2.22943842e-01 -1.84896946e-01  9.28160697e-02 -3.12536597e-01\n"," -2.75356621e-01 -6.11549951e-02 -4.62742485e-02  1.57129839e-01\n","  3.93826067e-01 -4.74612005e-02 -3.74386571e-02  2.69577980e-01\n","  6.84049129e-02 -1.40626878e-01  1.19964287e-01 -2.88004875e-01\n"," -8.72786343e-02 -5.39067946e-02 -1.91669054e-02  1.93379268e-01\n"," -1.34013310e-01  3.68573517e-01  1.72697797e-01 -2.02747881e-01\n","  1.45673454e-01  3.96165967e-01  6.62912205e-02  6.49530366e-02\n","  2.25084081e-01 -1.00023352e-01 -2.87286699e-01 -3.06217343e-01\n","  1.86905041e-01 -5.69505282e-02 -3.62192430e-02  2.09552981e-02\n"," -1.09786950e-02  9.68729705e-02  3.71543705e-01 -6.49446575e-03\n"," -2.74537921e-01  6.18739314e-02  6.90428242e-02  1.81765556e-01\n"," -4.96733189e-02 -2.27280751e-01  2.46602502e-02  1.74239308e-01\n","  3.65163237e-02 -2.08713442e-01 -2.09386006e-01 -4.60662603e-01\n"," -8.00162647e-03 -2.05779355e-02  2.67694801e-01  8.52164105e-02\n"," -1.46582216e-01  1.15911745e-01 -1.30515371e-03  6.31485693e-03\n","  1.91072002e-01 -2.03456253e-01 -1.61278099e-01 -2.55659640e-01\n"," -1.39230013e-01 -3.19399536e-01  3.32391053e-01  1.35183886e-01\n"," -4.97681051e-01 -8.70753303e-02  2.57772449e-02 -1.78807512e-01\n","  3.04723352e-01 -1.96859971e-01  9.75982323e-02  2.02711761e-01\n"," -2.80507684e-01 -2.30188817e-01 -1.38573065e-01 -1.98921729e-02\n","  2.68938005e-01 -3.69131863e-01  4.00899827e-01 -1.84183910e-01\n"," -3.27631533e-01 -1.75088048e-01 -6.05392549e-03 -2.73709577e-02\n"," -3.34389567e-01  1.76967785e-01  1.86404169e-01  1.07600763e-01\n","  2.10724995e-01  1.78879216e-01 -1.44967125e-04  4.03522626e-02\n","  1.09756529e-01  1.45661637e-01  7.37254098e-02  1.25067666e-01\n","  1.26089111e-01  3.53349410e-02 -4.97299343e-01 -1.05487093e-01\n"," -8.98510963e-02  1.66357800e-01  2.43499517e-01 -2.94598043e-01\n"," -1.69871658e-01  4.40371819e-02 -9.39229429e-02 -1.92125663e-01\n"," -8.53724498e-03 -3.71907622e-01  6.08349927e-02 -2.04594150e-01\n","  1.19889632e-01  1.48424938e-01 -1.32330675e-02  6.23622015e-02\n"," -3.35727334e-01  1.64463773e-01 -1.59562275e-01 -1.28980637e-01\n"," -2.32830316e-01  4.39768694e-02  1.29323959e-01  1.34830520e-01\n","  5.29366266e-03 -2.71381438e-01 -8.07575956e-02  2.42135391e-01\n"," -1.81095470e-02  3.85384448e-02  1.89926997e-01 -1.78553894e-01\n"," -1.67449281e-01 -2.36744985e-01 -2.76641935e-01  2.12435469e-01]\n"]}]},{"cell_type":"code","source":["# 5°) Le hacemos un LEN para ver la longitud que tiene el vector --->\n","\n","#len(modelo.get_vector(\"rusia\"))\n","# 400 posiciones\n","print(len(vector))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtlCn90HwNnD","executionInfo":{"status":"ok","timestamp":1720552305464,"user_tz":180,"elapsed":27,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"3fb948b4-2017-4c1c-bcf8-c417dc5f72f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["300\n"]}]},{"cell_type":"code","source":["# 6°) Como podemos comprobar este Vector y ver que es lo mas similar a Rusia, nos va a devolver las palabras que el considera\n","# se encuentra cercanas a rusia --->\n","\n","modelo.most_similar(\"rusia\") # Use most_similar method to find nearest neighbors\n"],"metadata":{"id":"s7i9CGzpwg0w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552308177,"user_tz":180,"elapsed":2722,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"594ad3b3-d8fb-4fa0-8646-8d433b5cf71e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('georgia', 0.6840266585350037),\n"," ('eeuu', 0.6735044717788696),\n"," ('sovietica', 0.6270561814308167),\n"," ('eslovenia', 0.6217913031578064),\n"," ('haiti', 0.6211448907852173),\n"," ('polonia', 0.6080873012542725),\n"," ('Kazakistán', 0.6035131812095642),\n"," ('Bielorussia', 0.5969798564910889),\n"," ('zelanda', 0.595187246799469),\n"," ('Prikhodko', 0.595004141330719)]"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTjI_GjcUigc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552308639,"user_tz":180,"elapsed":466,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"008f399f-b9c9-4f7b-e9ec-42b0c4d510f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('citroen', 0.8791922330856323),\n"," ('cdi', 0.8286747336387634),\n"," ('psa', 0.8272714018821716),\n"," ('tbn', 0.8142942786216736),\n"," ('xperia', 0.8097282648086548),\n"," ('kaDIGITO', 0.8069819211959839),\n"," ('daewoo', 0.8064446449279785),\n"," ('PHPSESSID', 0.8050963878631592),\n"," ('gcr', 0.8047224283218384),\n"," ('tkDIGITO', 0.7997612357139587)]"]},"metadata":{},"execution_count":62}],"source":["# 7°) Si hacemos lo mismo con un marca de auto o un pais de Europa hace la misma relacion ---->\n","\n","modelo.most_similar(\"peugeot\") # Use most_similar method to find nearest neighbors\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkyLG4YXVz91","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552308946,"user_tz":180,"elapsed":317,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"be51ec5b-3c4b-49b9-b667-1cc3ce50052a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('morgado', 0.646475613117218), ('rsssf', 0.6181958317756653), ('gddc', 0.6166597008705139), ('valdes', 0.615239143371582), ('elcciones', 0.6151246428489685), ('unilex', 0.6145886182785034), ('deptos', 0.6131942272186279), ('skiandorra', 0.6126061677932739), ('tabless', 0.6114614009857178), ('espanya', 0.6110906004905701)]\n"]}],"source":["palabras_similares = modelo.most_similar(\"portugal\") # Use most_similar method to find nearest neighbors\n","print(palabras_similares)"]},{"cell_type":"markdown","metadata":{"id":"zs-IdpGQWRUT"},"source":["###**EXPLORANDO RELACIONES ENTRE PALABRAS**\n","\n","Cuando hablamos de una representación vectorial, nos preguntamos: ¿será que yo puedo hacer operaciones entre estos vectores? Entonces, si nosotros nos acordamos, por ejemplo, de nuestras clases de matemáticas, cuando estábamos aprendiendo álgebra lineal, que hablábamos sobre vectores y matrices, sabemos que podemos sí operar los vectores.\n","\n","Vamos a utilizar un recurso que se llama **GeoGebra.rg**. Y vamos a entrar a esta página y vamos a utilizar la calculadora gráfica que ellos ofrecen. Entonces en esta calculadora gráfica pues yo tengo mi plano cartesiano, entonces supongamos que yo quiero establecer unos puntos en mi plano cartesiano, vengo a donde dice: herramientas y voy a colocar un punto. Entonces un punto.\n","Entonces seleccione una posición y colóquela. Entonces voy a colocar este punto aquí en esta posición. Y voy a seleccionar otro punto y voy a colocarlo en esta posición y voy a seleccionar a otro punto y voy a colocarlo, por ejemplo, en esta posición.\n","\n","Si yo vengo a donde dice álgebra en la parte superior, entonces yo tengo mis puntos y sus respectivas coordenadas. Entonces, tengo un espacio bidimensional, o sea de dos dimensiones. El eje de las X y de las Y, entonces vamos a cambiar aquí el punto. Supongamos que esos puntos son países, entonces el primer país podría ser, por ejemplo, Ecuador.\n","Y el otro país podríamos decir que es Colombia. Y digamos que este que está aquí digamos que es por ejemplo Argentina. Entonces, ¿yo qué podría hacer? Yo podría, por ejemplo tomar los puntos que corresponden a Ecuador y a Colombia y sumarlos.\n","\n","Entonces le doy a clic aquí donde dice + y le doy donde dice una expresión, entonces yo puedo decir simplemente Ecuador, que es un punto más Colombia. Y entonces me da un punto resultante, que sería éste de aquí que sería A. Yo también podría, si yo quiero hacer otra función aquí, una expresión, es decir, por ejemplo Ecuador – Colombia, y me mostraría otro punto.\n","Entonces, dependiendo de la operación que ejecute, yo voy a tener un resultado diferente al original, porque yo estoy digamos, tomando estos vectores y los estoy sumando y obtengo ese punto, y cuando los resto, obtengo este punto, entonces yo puedo operar los vectores entre sí. ¿Por qué hacemos esto?\n","\n","Porque al hacer estas operaciones vamos a obtener vectores resultantes y estos vectores resultantes tienen la propiedad de que van a estar en la misma región. Entonces veamos en Python cómo podemos hacer esto con nuestros vectores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ql1TRnNxWhQK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552309313,"user_tz":180,"elapsed":369,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"6fb103f0-8d30-4b2a-a25e-c2dfdc9ddeed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('america', 0.6284738183021545),\n"," ('uruguay', 0.6268236637115479),\n"," ('bucaramanga', 0.6031538844108582),\n"," ('atletico', 0.5954282879829407),\n"," ('postobon', 0.5931621193885803),\n"," ('carchi', 0.5929304957389832),\n"," ('bolivia', 0.5916794538497925),\n"," ('cartagena', 0.5915964245796204),\n"," ('cauca', 0.5915430784225464),\n"," ('sudamerica', 0.5883246660232544)]"]},"metadata":{},"execution_count":64}],"source":["# 1°) Vemos como lo podemos hacer con PYTHON --->\n","\n","modelo.most_similar(positive=[\"colombia\", \"ecuador\"])\n","\n","# Nos devuelve el grado de similaridad entra ellas en un orden decreciente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFDTx446WhSm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552309678,"user_tz":180,"elapsed":367,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"6b1d31c1-8945-48b4-885e-aeb4e9a88645"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('milan', 0.7197256684303284),\n"," ('formulaDIGITO', 0.6900485754013062),\n"," ('atletico', 0.6868566870689392),\n"," ('bayern', 0.683440089225769),\n"," ('supersport', 0.682165801525116),\n"," ('uefa', 0.6795853972434998),\n"," ('whitney', 0.6774600744247437),\n"," ('dakar', 0.6753880381584167),\n"," ('audino', 0.6749749779701233),\n"," ('girone', 0.67469322681427)]"]},"metadata":{},"execution_count":65}],"source":["# 2°) Ahora tratemos de hacer esas compraciones entre objetos diferentes --->\n","\n","modelo.most_similar(positive=[\"italia\", \"ferrari\"])\n","\n","# Nos devuelve semejanza, No es tanta ya que Ferrari es italiana marca de auto, no nos devuelve tanta semejanza."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGvEvT8AWhVN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552310047,"user_tz":180,"elapsed":371,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"90a15cb6-47f1-4b1a-a2bd-e2da0989bf56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Stormyran', 0.096382737159729),\n"," ('UWG', 0.08730211853981018),\n"," ('acuífero', 0.034277476370334625),\n"," ('acuíferos', 0.020184578374028206),\n"," ('segregado', 0.01361220795661211),\n"," ('segregada', 0.009701022878289223),\n"," ('drenan', 0.00539777148514986),\n"," ('aluvial', 0.0030193119309842587),\n"," ('MCP', 0.0027628433890640736),\n"," ('Trasvase', 0.0026574123185127974)]"]},"metadata":{},"execution_count":66}],"source":["# 3°) Ahora hacemos otro ejemlo pero con NEGATIVE que aleja la similiridad entre los Vectores que asignamos --->\n","\n","modelo.most_similar(negative=[\"italia\", \"ferrari\"])\n","\n","# Nos devuelve similiridad que NO tiene sentido con Italia y Ferrari"]},{"cell_type":"markdown","source":["***Con Negative nos devuelve cosas totalmente que no tienen sentido. Entonces ‘kenbugul’, ‘llenaba’, ‘armadura’, ‘protegía’, ‘liberaba’, ‘llenaban’, ‘oprimía’, ‘arrojaba’, ‘bubónicos’. Entonces aquí, yo por ejemplo al utilizar estas funciones de que si es para buscar más semejanza, o digamos buscar las palabras más distantes, entonces podemos nosotros operar los vectores y ellos van a encontrarse en una región, recordemos que estamos en un espacio vectorial de 400 dimensiones.***\n","\n","***Entonces se puede hacer estas operaciones de modo que pueden hallarse estos patrones entre nuestros vectores.***"],"metadata":{"id":"s3_uDwG6uIIE"}},{"cell_type":"markdown","metadata":{"id":"iEYqPrFOhQ1B"},"source":["###**PROFUNDIZACION DE RELACIONES ENTRE PALABRAS**\n","\n","Profundicemos un poco más en este tema tan interesante. Vamos a crear más líneas de código. Y, por ejemplo, vamos a crear también inclusive un pequeño texto aquí. Si yo digo por ejemplo, lo siguiente, ***la siguiente relación: nube : nubes :: estrella : estrellas***. Entonces, digamos si yo sé que de nube es a nubes como estrella es a estrellas, por ejemplo en mi cabeza, pues yo ya sé esta relación.\n","\n","Yo sé que aquí estoy hablando de número, de coincidencia, que coinciden números. Entonces la nube es a nubes como estrella es a estrellas. Por ejemplo, otra. ***Carro : transporte :: casa ¿qué se les ocurre a ustedes? Vivienda, bueno. Por ejemplo, casa es a vivienda o carro es a transporte como casa es a vivienda, por ejemplo.***\n","\n","Entonces, estas relaciones, ¿será que yo las puedo de alguna forma establecer a través de estos métodos vectoriales? Bueno, vamos a ver el ejemplo más detenidamente. Entonces, por ejemplo, si yo tengo el siguiente: modelo.most_similar. Entonces, si yo hago lo siguiente, si yo le hago positive, si yo sumo por ejemplo. So yo sumo los vectores por ejemplo ‘nube’ y ‘estrella’.\n","Y también le quiero restar por ejemplo, nubes. ¿Entonces, cuál sería mi vector resultante? Entonces vean ustedes una cosa. Volvamos aquí. Si yo pongo nube + estrellas - nube. ¿Ustedes qué se imaginan cuál debería ser la respuesta? Nubes. Perdón, nubes + estrella – nube. Ahora sí tiene sentido.\n","\n","Entonces yo estoy sumando, digamos nubes, en plural, un sustantivo en plural más uno en singular, menos otro en singular, que tiene que ver con esta palabra. Entonces aquí automáticamente, y estoy viendo, es como un plural, buscando un plural. Mi respuesta, lo más lógico es que me diga que son estrellas.\n","Si ven la relación, yo tengo nubes, ver a una palabra en plural, más estrella, que es en singular menos el singular de nubes, que sería nube, entonces, lo más lógico es que yo tenga el plural de estrella.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7SAriBkEQRg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552310356,"user_tz":180,"elapsed":312,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"c5b72f52-a0d6-47c5-f175-2ec313744c97"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('supergigantes', 0.5424259305000305),\n"," ('astros', 0.5286556482315063),\n"," ('nebulosas', 0.515497624874115),\n"," ('eclipsantes', 0.5148696303367615),\n"," ('subenanas', 0.5124854445457458),\n"," ('estelar', 0.5027156472206116),\n"," ('supermasivas', 0.5004077553749084),\n"," ('luminosidades', 0.49870434403419495),\n"," ('enanas', 0.49432995915412903),\n"," ('Cefeidas', 0.4911353588104248)]"]},"metadata":{},"execution_count":67}],"source":["# 1°) Establecemos las relaciones entres los Vectores --->\n","\n","modelo.most_similar(positive=[\"nubes\", \"estrellas\"], negative=[\"nube\"])\n","\n","# El algoritmo encuentra esa relacion Plural."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAGpkb9UEQUd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552310695,"user_tz":180,"elapsed":344,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"f9ae7b61-342f-417e-a3f7-016548a34588"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('profesora', 0.8312232494354248),\n"," ('catedrática', 0.7350810766220093),\n"," ('Profesora', 0.7213646769523621),\n"," ('catedrático', 0.6825595498085022),\n"," ('doctorada', 0.6590282320976257),\n"," ('cátedra', 0.645167887210846),\n"," ('Catedrática', 0.6417908668518066),\n"," ('emérita', 0.6316346526145935),\n"," ('Profesor', 0.6310397386550903),\n"," ('doctora', 0.6257742643356323)]"]},"metadata":{},"execution_count":68}],"source":["# 2°) El Algoritmo tambien lo reconoce en genero --->\n","\n","modelo.most_similar(positive=[\"mujer\", \"profesor\"], negative=[\"hombre\"]) # Le restamos negative.\n","\n","# Nos devuelve genero Femenino"]},{"cell_type":"code","source":["modelo.most_similar(positive=[\"mujer\", \"medico\"], negative=[\"hombre\"]) # Le resto negative.\n","\n","# Le quitamos hombre y nos devuelve alguna profesiones asociadas a mujer y medico, pero tambien nos devuelve\n","# asociaciones que el algoritmo NO es capaz de hallar, como por ejemplo: \"niñera\", \"amiga\"."],"metadata":{"id":"-3CUl7ju0rXy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720552311137,"user_tz":180,"elapsed":449,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"536780bf-1401-4adc-bc6c-90c1d92d86e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('clinica', 0.5922838449478149),\n"," ('enfermera', 0.5733241438865662),\n"," ('partera', 0.5719038248062134),\n"," ('médico', 0.5637348294258118),\n"," ('obstetriz', 0.5626001954078674),\n"," ('ginecológica', 0.5597516894340515),\n"," ('tocólogo', 0.5474926829338074),\n"," ('gineco', 0.5469223856925964),\n"," ('obstetra', 0.5452975630760193),\n"," ('obstetricia', 0.539606511592865)]"]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["***El modelo no es perfecto en primer lugar. Y por otro lado, el modelo tiene un bies, recordemos que estamos hablando de redes neuronales, en la red cada palabra, cada entrada tiene un peso y un bies asociado a ellas, y este bies entonces, de acuerdo a lo que se observa en el contenido con el cual se entrenan los modelos, hace que él no sea capaz de hallar estas relaciones.\n","Entonces, a veces por motivos de religión, de política, de género, de orientación de la persona, la parte de orientación sexual, todas estas cosas entonces el modelo puede tener un bies.***\n","\n","***Entonces estos bieses digamos son asuntos de mucha relevancia en el área del procesamiento del lenguaje humano y hay mucha investigación al respecto para tratar de reducir este bies, porque finalmente los datos son el producto de lo que somos, de nuestra cultura, de lo que nosotros hemos ido construyendo como sociedad.***\n","***Entonces el bies siempre va a existir, pero la idea es que, en la medida que nuestros modelos evolucionen entonces, entonces este bies o este sesgo se disminuya. Entonces queríamos mostrar esta relación para que ustedes vean que de todas maneras, pues en el modelo, a pesar de ser muy preciso, pues no es perfecto.***"],"metadata":{"id":"lrFB6jUt1YEm"}},{"cell_type":"markdown","source":["***Dato del Aula: SESGO EN LOS DATOS***\n","\n","Como lo discutimos en el aula, la Inteligencia Artificial (IA) es una de las áreas con mayor potencial de transformar el modo como vivimos, la movilidad y la salud. Con el impulso y progreso de la IA durante los últimos años, ha sido posible el avance tecnológico en materia de hardware y un aumento exponencial de datos disponibles para entrenamiento de los modelos de aprendizaje de máquina.\n","\n","Teniendo en cuenta que los modelos más promisorios en esta área dependen mucho de los datos que producimos, estos modelos pueden reproducir estereotipos de nuestra sociedad, impulsando aún más la desigualdad social. Desarrollar modelos que no reproduzcan los aspectos negativos de la sociedad es crítico para garantizar un futuro más justo para todos; por ello, áreas de investigación como ética en la IA han venido creciendo considerablemente en los últimos años.\n","\n","Queremos recomendarte que leas este artículo presentado en el encuentro anual del Foro Económico Mundial, escrito por Richard Socher, científico jefe de Salesforce, que aborda de manera muy interesante los peligros involucrados en la reproductibilidad de estereótipos humanos en sistemas de IA.\n","\n","Otra gran referencia bibliográfica es el capítulo 6, sección 6.11 - Bias and Embeddings, del libro Speech and Language Processing escrito por Dan Jurafsky y James H. Martin. En esta sección, ellos abordan el tema del sesgo, además de traer referencias con técnicas para reducir este problema en el embedding de palabras."],"metadata":{"id":"gW1G6f6a1pmU"}},{"cell_type":"markdown","metadata":{"id":"QmzHCHuJEQmd"},"source":["#<font color=red>**COMBINANDO VECTORES**</font>\n","\n","---"]},{"cell_type":"markdown","source":["###**TOKENIZACION**\n","\n","* 1. Obtener noticias, ya fueron cargados nuetros dataset, utilizamos principalmente el titular.\n","\n","* 2. Realizar la Vectorizacion de los titulares, primero Vectorizamos las palabras y luego las operamos para convertir otras frases en un solo vector.\n","\n","* 3. Por ultimo hacemos la Clasificacion."],"metadata":{"id":"A-a6j1zF3YVo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrJsCW4uEZLW","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1720552311139,"user_tz":180,"elapsed":29,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"84e7e179-4a65-45ab-809c-de2c17b6a79a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'¡Cuánta humildad Eva Longoria! La captan comiendo taquitos de cochinita pibil en mercado de Mérida'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}],"source":["# 1°) Hecemos LOC del titulo del entrenamiento y ponemos un numero para ver que titulo nos muestra --->\n","\n","entrenamiento.titulo.loc[5643]\n","\n","# Nos devuelve un titular: ¡ Cuanta humildad Eva ...! La captan comiendo..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hc2EXOW_EZ6T","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1720552311144,"user_tz":180,"elapsed":31,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"8485c1d3-ea2f-4a35-8d47-afcb4c57c49b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'mundo'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":71}],"source":["# 2°) Ahora vamos a ver como la Categorizo el diario --->\n","\n","entrenamiento.categoria.loc[5643]\n","\n","# Nos devuelve la Ctegoria Mundo."]},{"cell_type":"markdown","source":["La idea es tomar nuestro titular y TOKENIZAR cada una de las palabras y asignarles un Vector y al hacer esto podemos operar nuestros Vectores entre si para obtener 1 solo Vector que represente ese titulo. Y con esa informacion vamos a poder proceder a entrenar nuestros modelos."],"metadata":{"id":"W3bduMcWGa9-"}},{"cell_type":"markdown","metadata":{"id":"58RKUvJtEaO8"},"source":["###**CONTRUYENDO EL TOKENIZADOR**\n","\n","Vamos a tomar nuestras frases y la separamos por palabras (Tokenizar) y utiizamos NLTK."]},{"cell_type":"code","source":["# 1°) Definimos una Funcion Tokenizador ---->\n","\n","import nltk\n","nltk.download(\"punkt\")\n","import string\n","\n","def tokenizador(texto): # Recibe parametro texto y nos va a devolver una Lista con Token\n","  texto = texto.lower().replace(\"¡\", \" \").replace(\"¿\", \" \") # Tomamos palabras en minuscula solamente.\n","  lista_alfanumerica = [] # Lista vacia para almacenar palabras y nuemros\n","\n","  for token_valido in nltk.word_tokenize(texto):\n","    if token_valido in string.punctuation:\n","      continue # Que continue si encuntra puntuaciones o signos\n","    lista_alfanumerica.append(token_valido)\n","  return lista_alfanumerica"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8k3MBy1VTA-","executionInfo":{"status":"ok","timestamp":1720552311637,"user_tz":180,"elapsed":521,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"3b91c269-e84a-4f2a-881c-1feeea213123"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["# 2°) Llamamos la funcion y colocamos el STRING del titulo de la noticia de mas arriba --->\n","\n","tokenizador(entrenamiento.titulo.loc[5643])\n","\n","# No remueve el sigo de apertura de Exclamacion, esto sucede porque al ser español no reconoce la funcion\n","# los signos de apertura que se usan solo en el español, para esto tenemos que agregar en la funcion\n","# puntualmente que signos queremos que nos borre ---> texto = texto.lower().replace(\"¡\", \" \").replace(\"¿\", \" \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDcitxR0W0Kj","executionInfo":{"status":"ok","timestamp":1720552311638,"user_tz":180,"elapsed":20,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"64367876-ebc1-4726-bcdf-d756fd17c355"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['cuánta',\n"," 'humildad',\n"," 'eva',\n"," 'longoria',\n"," 'la',\n"," 'captan',\n"," 'comiendo',\n"," 'taquitos',\n"," 'de',\n"," 'cochinita',\n"," 'pibil',\n"," 'en',\n"," 'mercado',\n"," 'de',\n"," 'mérida']"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["# Si hacemos un print de la funcion STRING.PUNCTUATION podemos ver que signos reconoce --->\n","\n","print(string.punctuation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3RaA53CCXgzi","executionInfo":{"status":"ok","timestamp":1720552311638,"user_tz":180,"elapsed":17,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"94c4a893-7c05-4be4-c24e-b9ec921600b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"]}]},{"cell_type":"markdown","source":["#<font color=red>**COMBINANDO VECTORES**</font>\n","\n","---\n","\n","Ya pudimos Vectorizar cada una de las palabras de nuestros textos, pero ahora tenemos que crear un VECTOR para que nos permita identificar todas las frases, un Vector de texto como tal, y el texto van a ser los titulares de las noticias."],"metadata":{"id":"eV4JJK38Q3iO"}},{"cell_type":"markdown","source":["###**SUMA DE VECTORES**"],"metadata":{"id":"dya0lXfIaIYl"}},{"cell_type":"code","source":["# 1°) Importamos la biblioteca Numpy y vamos a definir una funcion de combinacion de vectores por suma\n","# y va a recibir Tokens --->\n","\n","import numpy as np\n","\n","def combinacion_vectores_por_suma(tokens):\n","  vector_resultante = np.zeros(300) # Matriz de 0 ceros de las 300 posiciones del modelo\n","  for token in tokens:\n","    try:\n","      vector_resultante += modelo.get_vector(token)\n","    except KeyError:\n","      continue\n","  return vector_resultante"],"metadata":{"id":"DLURr5vmQ9Vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2°) Ahora creamos una variable palabra para pasar una frase de prueba y luego hacemos una combinacion de vectores\n","# por suma y colocamos nuestros Tokens --->\n","\n","palabras = tokenizador(\"esta es una prueba de el tokenizador\")\n","vector_texto = combinacion_vectores_por_suma(palabras)\n","print(len(vector_texto))\n","print(vector_texto)\n","\n","# Sin el TRY/EXCEPT en la funcion nos devuelve un KeyError ya que la palabra TOKENIZADOR no esta en el vocabulario, esta\n","# palabra es una adaptacion del Ingles para el español, NO exite en el modelo de de Word Embbeding que usamos. Para evitar\n","# esto debemos agregar TRY/EXCEPT en la funcion.\n","\n","# Luego de hacer esto nos devuelve los Vectores de esa frase."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RluWj2xQ9ds","executionInfo":{"status":"ok","timestamp":1720552311639,"user_tz":180,"elapsed":12,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"4ea76944-500c-424b-a219-1cf2e994174c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["300\n","[ 2.76417430e-01 -1.58912602e-01  1.12052033e+00 -5.80981908e-01\n"," -2.32586863e-01  7.06117500e-01  1.94477333e-01 -5.48061430e-02\n","  1.80467623e+00 -3.39351873e-01 -5.37948150e-01 -3.26756693e-01\n","  6.88175907e-01 -7.52478324e-01 -1.61008080e-01 -5.10633826e-01\n","  2.62305342e-01 -2.32391208e-01 -5.13517614e-01  1.01253449e+00\n"," -6.57654399e-01 -1.13863767e+00  4.43006668e-01  1.69649734e+00\n"," -4.86636002e-01 -9.83828921e-02  4.12313228e-01  1.04334123e+00\n","  4.96658640e-01  1.56617989e+00  7.66229037e-01 -6.01797348e-01\n"," -1.00906293e+00  3.70567856e-01  7.21053603e-01 -6.01695210e-01\n"," -4.53551553e-01 -4.35301573e-01 -8.57206687e-01 -1.49931391e-02\n","  2.42533080e-01  1.52306220e-01 -1.11986350e+00 -6.24655637e-01\n"," -7.66971007e-01 -1.03714629e+00  8.54260549e-02 -1.41196406e-01\n","  8.91030712e-01  1.60064513e+00 -4.69779670e-01  1.00350538e+00\n","  2.84894608e-01  2.05808887e+00 -1.08080504e-01  1.43592360e-01\n"," -8.55967004e-01 -1.22604439e+00  7.46420473e-02  7.70092987e-01\n","  6.69612825e-01  5.52351341e-01 -6.76020509e-01 -6.97424786e-01\n","  1.77442729e-02  5.62045158e-01 -5.01611588e-01  8.62711413e-01\n"," -1.12367373e+00  4.66198023e-01 -1.26922062e+00 -2.81421870e-01\n","  6.77755499e-01  6.53380770e-01 -7.49803171e-01  9.67004336e-01\n","  9.65019353e-02  7.53804501e-01 -8.21908697e-01 -2.64217381e-01\n","  6.35071394e-01 -1.26202555e-01  5.05769899e-01  4.62212129e-01\n","  3.63551810e-01 -4.59715437e-01 -2.28293367e-01 -4.83098067e-02\n","  5.78731051e-01  4.11667164e-01 -1.41953226e-01 -5.73967475e-01\n","  2.47275640e-01  4.30302710e-01 -1.59025604e+00 -2.91749786e-01\n","  3.46002154e-01 -1.45234889e-01  4.04568590e-01  1.09835243e+00\n"," -6.85794026e-01 -4.99877613e-01 -7.65987433e-01  2.08566631e+00\n","  9.56498280e-01 -7.66242698e-01 -6.85315076e-02 -1.93334989e+00\n","  3.94824080e-01  9.01342018e-01  9.65655062e-01  5.26115688e-01\n","  5.97220227e-01  1.94878622e-01 -5.12336157e-01 -1.00495525e+00\n","  6.49965476e-01 -3.56326649e-01 -8.20165507e-01 -1.13298306e+00\n","  5.24970017e-01 -4.09930117e-01 -8.57507281e-01  3.04925422e-01\n"," -4.86026187e-01  4.72709162e-01 -3.15619884e-01  3.03497114e-01\n","  7.71404382e-01 -2.94547834e-01  8.79535340e-01  3.46982719e-01\n"," -1.15150452e+00 -8.24334399e-01 -5.69819904e-01  8.34477207e-01\n"," -6.35618251e-02 -1.40916837e+00  7.46261895e-01 -5.91334645e-02\n"," -5.85849108e-02  1.17892131e-01 -7.03953337e-01 -7.75396144e-02\n"," -6.00789860e-03 -2.67491890e-02  4.35258396e-01 -3.93141189e-01\n","  1.52084579e+00 -1.16902082e+00  4.91794955e-01  1.68875221e-01\n"," -6.72675457e-01 -9.02376380e-01 -3.47979867e-01 -4.19645655e-01\n","  1.37452406e+00  4.53467030e-01 -7.67611064e-01 -1.88559659e-01\n","  5.68243255e-01 -8.02699173e-01  1.39043232e+00 -5.36503978e-02\n"," -4.69844643e-01 -1.08273696e+00 -3.15802554e-01  6.21373717e-01\n","  7.07253881e-01 -1.16225446e+00  1.22398424e+00 -1.32611462e+00\n","  2.86725976e-01  2.00113356e-02  8.34184473e-01 -2.08242574e+00\n","  1.92547878e-02 -3.30660877e-01 -2.96550290e-01  3.72832315e-01\n","  5.89724750e-01  1.48047620e+00  1.26068690e+00  2.54262673e-01\n"," -6.59951650e-01  4.16804031e-02  2.80219277e-01 -7.90879715e-01\n"," -3.76226321e-01 -1.49937602e-01 -9.40475591e-01  9.47881952e-01\n","  9.89891425e-01 -6.16241097e-01  2.91367846e-01  1.29919016e+00\n","  2.97017463e-01  1.26512596e+00 -2.08140027e-01  1.42048057e-01\n"," -1.80613796e+00 -1.77476759e-01 -4.25636370e-01 -2.31932506e-01\n"," -8.58032955e-01 -1.53733987e+00  1.94703333e-01  1.79235500e-01\n","  7.58111382e-01  9.82548073e-01  5.37932105e-02 -7.72191699e-01\n"," -3.73490525e-01 -2.54149687e-01  6.56926967e-01  1.08485660e+00\n"," -4.45243869e-01  1.33085218e+00  2.54041840e-01  8.95208200e-01\n"," -3.06790036e-01 -6.66768534e-01  1.20496291e+00  2.16383487e-05\n"," -1.21942409e+00  6.29116868e-01  5.37862303e-01 -2.18186093e-01\n"," -1.32215612e+00 -9.09802684e-01  4.69226302e-01  8.69622944e-01\n","  3.87752675e-02  1.10685153e+00 -1.35039090e+00  6.76595634e-01\n","  4.77682941e-01 -2.71115914e-01 -6.53001186e-01  6.08673213e-01\n","  2.36098616e+00 -1.59315764e-01  7.14361484e-02 -8.41039654e-01\n"," -1.28428096e+00  2.29581892e-02 -1.27348447e+00 -2.73574032e-02\n","  1.46713529e-01  4.38896129e-01  3.88904486e-01  1.36170889e+00\n","  3.84401340e-01  7.93386046e-02 -7.56174255e-01 -6.03177205e-01\n","  1.77162586e-01  3.88347496e-01 -9.80575860e-01 -8.97795744e-01\n","  3.61146916e-01  5.28897724e-01 -1.14813091e-01 -6.35289580e-01\n","  3.64533555e-01  3.18483810e-02 -2.58738716e-01  4.90236294e-01\n"," -2.32874776e-01  1.85268223e+00  2.81229503e-01  4.78403417e-01\n","  8.13311752e-01  2.30492640e-01 -2.81480316e-01  3.59319802e-03\n","  6.05277988e-01  3.73338403e-01 -1.07289683e-02 -1.15698095e+00\n"," -1.15968193e+00  3.90895456e-03 -4.45661348e-01  8.93968912e-02\n","  1.24134687e+00  5.44230999e-01  7.34045647e-01 -2.59966521e-01\n"," -4.71730728e-01 -1.09015173e+00  1.76501703e-02  7.41977915e-01\n"," -4.26463792e-02 -2.81933531e-01 -3.33300211e-01 -1.40268816e+00\n"," -4.81586754e-01 -1.34737553e+00 -1.75183314e-01  5.73159389e-01]\n"]}]},{"cell_type":"markdown","source":["#<font color=red>**CLASIFICACION CON WORD2VEC**</font>\n","\n","---"],"metadata":{"id":"GeqSLxw8Zxiq"}},{"cell_type":"markdown","source":["###**VECTORIZANDO BASE DE ENTRENAMIENTO Y TEST**\n","\n","Vamos a crear nuestra Matriz con los textos, con esta matriz vamos a poder obtener nuestros respectivos Vectores para hacer nuestras Test y Entrenamiento."],"metadata":{"id":"wmMLDq7OZxlc"}},{"cell_type":"code","source":["# 1°) Vamos a definir una funcion para crear nuestra Matriz y despues la aplicamos al dataset de Entrenamiento y Test de las noticias\n","# para de esta manera obtener vectores de frases --->\n","\n","def matriz(textos):\n","  x = len(textos) # Longitud de los textos.\n","  y = 300\n","  matriz = np.zeros((x,y))\n","  for i in range(x):\n","    palabras_numeros = tokenizador(textos.iloc[i])\n","    matriz[i] = combinacion_vectores_por_suma(palabras_numeros)\n","  return matriz\n","\n","# Esta funcion toma la longitud de los textos para los valores de x(cantidad de noticias) por 300 que son las dimensiones de\n","# nuestro modelo, creamos la matriz y a cada uno de los espacios de esa matriz le vamos a aplicar el Tokenizador para hacerlo\n","# a los textos y despues haciendo la combinacion de vectores por suma de cada uan de estas palabras Tokenizadas.\n"],"metadata":{"id":"0mAF87icaTp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2°) Ahora procedemos a crear nuestra Matriz de Vectores para Entrenamiento y Matriz de Vectores para Test ---->\n","\n","matriz_train = matriz(entrenamiento.titulo)\n","matriz_test = matriz(test.titulo)\n","\n","print(matriz_train.shape)\n","print(matriz_test.shape)\n","\n","# Nos devuelve las filas y columnas."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06vF0sXXgRii","executionInfo":{"status":"ok","timestamp":1720552344003,"user_tz":180,"elapsed":32370,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"7385e805-867d-420c-9f03-52b70ebd969a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(91844, 300)\n","(22961, 300)\n"]}]},{"cell_type":"markdown","source":["***Dos matrices que ya tienen que tener 400 columnas las dos, y lo que va a variar es el número de filas de cada una de ellas.***\n","\n","***Entonces va a ser una cantidad de filas para el entrenamiento que van a ser 90000 o algo así y la de pruebas, que va a ser 20000 o algo parecido. Entonces ella va a hacer esta separación y va a aplicarle lógicamente las funciones para hacer los tokens en cada una de las palabras, de los textos, y después combinar estos tokens a través de una suma vectorial de los mismos para obtener el titular con un vector asociado a ese titular.***\n","\n","***Entonces él se va a demorar algunos momentos haciendo todo este esta ejecución. Y el próximo video entonces vamos a generar una baseline, vamos a utilizar un clasificador para poder tener como un marco de referencia y, posteriormente, utilizar un clasificador que será una regresión logística para entonces colocar a prueba nuestro modelo.***\n","\n","***Noten ustedes que ya nos trajo el resultado, entonces la matriz de entrenamiento tiene 91844 filas por 400, y la de prueba tiene 22961 filas por 400 columnas. Entonces ya estamos listos para seguir a nuestro próximo paso, que es nuestro baseline, ya nos vemos.***\n","\n"],"metadata":{"id":"6zMOefQbgSJz"}},{"cell_type":"markdown","source":["**Dato:**\n","\n","***Métricas de evaluación***\n","\n","Discutiremos algunas formas de evaluar un modelo de clasificación, detallando lo que es precisión (del inglés precision) y exhaustividad (del inglés recall). Sobre estos criterios de evaluación:\n","\n","***¿Cuál es la diferencia entre precisión y exhaustividad?***\n","\n","* La **Precisión** muestra el número de clases clasificadas correctamente sobre todo lo que fue clasificado como dicha clase, o sea, es la cantidad de verdadero-positivo sobre verdadero-positivo + falso-positivo. Por otro lado, la **Exhaustividad** es todo lo que fue clasificado correctamente sobre el total de aquella clase, es decir, la cantidad de verdadero-positivo sobre verdadero-positivo + falso-negativo. Entonces, la exhaustividad muestra la capacidad de recuperar clases relevantes, mientras que la precisión muestra qué tan bien estas clases son recuperadas.\n","\n"," La precisión y la exhaustividad son métricas importantes en el proceso de clasificación, pues de nada sirve ser preciso sin ser completo."],"metadata":{"id":"Yn3gxWIijBKj"}},{"cell_type":"markdown","source":["###**BASELINE CON DUMMY CLASSIFIER**\n","\n","Una vez que tenemos nuestras Matriz de Entrenamiento y Test listos vamos  acrear nuestra Baseline y para ello vamos a utilizar un recurso de SKLEARN llamado DUMMY CLASSIFIER que es un clasificador que nos permite tener como un marco de referencia para ver si el logra identificar algunos patrones y posteriormente con este Baseline vamos a poder colocar un clasificador como una REGRESSION LOGISTICA para tener un punto de comparacion.\n"],"metadata":{"id":"_-fOCBTMjuWj"}},{"cell_type":"code","source":["# 1°) Importamos el recurso, lo instanciamos y hacemos un fit --->\n","\n","from sklearn.dummy import DummyClassifier\n","\n","DC = DummyClassifier() #Instanciamos\n","DC.fit(matriz_train, entrenamiento.categoria) # x=titulo, y=categoria, de esta manera el va a aparender que cada uno de los titulares\n","# corresponde a una categoria especifica.\n","label_predict_dc = DC.predict(matriz_test) #Prediccion de la respectiva Categoria del Label que le va dar el modelo de Dummy."],"metadata":{"id":"pFT7K2ZWHul6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2°) Vamos a ver cuantas Categorias tenemos en el dataset de Prueba --->\n","\n","test.categoria.unique()\n","\n","# Nos devuelve 6 categorias."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BwEAvX_qHuoa","executionInfo":{"status":"ok","timestamp":1720552344004,"user_tz":180,"elapsed":58,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"4475ab5f-54b7-4ef0-ed2c-0c925a8b9b68"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['ciencia y tecnologia', 'entretenimiento', 'mundo', 'politica',\n","       'economia', 'deportes'], dtype=object)"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["# 3°) Creamos un reporte que nos va a mostrar la Precision, Recall, todo el puntaje, media, todo el resultado de las Metricas para ver que\n","# tan acertado el es --->\n","\n","from sklearn.metrics import classification_report\n","\n","CR_dummy = classification_report(test.categoria, label_predict_dc) # Instanciamos, compara si el clasificador fue acertado o no.\n","print(CR_dummy)\n","\n","# Con estos resultados nos arroja un WARNING:UNDIFINED METRIC WARNNING, nos dice que como hay 0.00 en algunos valores al momento de obtener\n","# estos resultados se van a ver afectados ya que NO debe haber 0.00 en el denominador para obtener de F1-score que es el resultado de la\n","# division de precission y recall, ya que esto tenderia a infinito y matematicamente es inviable.\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avJpxh6_Q8zP","executionInfo":{"status":"ok","timestamp":1720552344421,"user_tz":180,"elapsed":464,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"0097ad70-78c6-47bd-9fba-2d7ce706b847"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["                      precision    recall  f1-score   support\n","\n","ciencia y tecnologia       0.00      0.00      0.00      2864\n","            deportes       0.00      0.00      0.00      4277\n","            economia       0.24      1.00      0.38      5461\n","     entretenimiento       0.00      0.00      0.00      3580\n","               mundo       0.00      0.00      0.00      2838\n","            politica       0.00      0.00      0.00      3941\n","\n","            accuracy                           0.24     22961\n","           macro avg       0.04      0.17      0.06     22961\n","        weighted avg       0.06      0.24      0.09     22961\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["**Definicion:**\n","\n","***CBOW vs. Skip-Gram***\n","\n","Sobre las diferencias entre las arquitecturas de entrenamiento CBOW y Skip-Gram, responde:\n","\n","***¿Qué cambia en la manera de entrenar las representaciones vectoriales entre CBOW y Skip-Gram?***\n","\n","* **CBOW** es entrenado para realizar la previsión de una determinada palabra teniendo el contexto como información de entrada. Por su parte, **Skip-Gram** necesita prever el contexto recibiendo como input una palabra. Ambos generan un vector denso de tamaño predeterminado.\n","\n"," Esta es la diferencia de entrenamiento entre Skip-Gram y CBOW."],"metadata":{"id":"7ai0DXtbSZ3X"}},{"cell_type":"markdown","source":["###**CLASIFICANDO CON REGRESION LOGISTICA**\n","\n","Vamos a trabajar con este clasificador para poner a prueba nuestro modelo."],"metadata":{"id":"nss6nUd7SNOs"}},{"cell_type":"code","source":["# 1°) Importamos el clasificador, instanciamos, tiene una serie de parametros que vamos a intentar mantener lo mas estandar porsible, menos un parametro\n","# que vamos a modificar que es el numero de Iteraciones MAX_ITER, parque Wird2Vec pueda generalizar bien debe tener un numero alto de Iteraciones,\n","# y luego hacemos un Fit ---->\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","LR = LogisticRegression(max_iter=1000) # 1000 iteraciones\n","LR.fit(matriz_train, entrenamiento.categoria)\n","\n","# Nos devuelve que el modelo con 1000 iteraciones NO puede convergir, en mi caso si pudo ejecutarse el calsificador, puede ser por el que el modelo\n","# de Word Embbeding que esto utilizando tiene 300 posiciones y no 400 como la que nos dio el aula que no funcion el archivo."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"6-4Uri6WSWIm","executionInfo":{"status":"ok","timestamp":1720552421022,"user_tz":180,"elapsed":76607,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"3a50c2d3-81e2-4c8a-9375-4e6e3c822155"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(max_iter=1000)"],"text/html":["<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["# Probamos con 1500 Iteraciones --->\n","\n","LR = LogisticRegression(max_iter=1500) # 1500 iteraciones\n","LR.fit(matriz_train, entrenamiento.categoria)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"rv_XZlGjSWLa","executionInfo":{"status":"ok","timestamp":1720552537946,"user_tz":180,"elapsed":116932,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"9a9b2623-a5c7-4444-fef6-008aa90c977a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(max_iter=1500)"],"text/html":["<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1500)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":83}]},{"cell_type":"markdown","source":["**Dato:**\n","\n","***Para saber más: regresión logística***\n","\n","En esta aula utilizamos la Regresión Logística (RL) para realizar la clasificación de los titulares.\n","\n"],"metadata":{"id":"RBHP0EYW5dRU"}},{"cell_type":"markdown","source":["###**CLASIFICANDO CON REGRESION LOGISTICA PARTE 2**\n","\n","Una vez que se logro Instanciar."],"metadata":{"id":"VFCNInnD5q8X"}},{"cell_type":"code","source":["# 1°) Vamos a ver de cuantas Iteraciones necesito el parametro MAX_ITER para poder convergir --->\n","\n","LR.n_iter_\n","\n","# Ese numero de iteraciones necesito para poder convergir, para poder generalizar."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQweC7Wc5vZx","executionInfo":{"status":"ok","timestamp":1720552550196,"user_tz":180,"elapsed":335,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"74084071-44df-44c1-85fb-e0ce43e580ce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([398], dtype=int32)"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["# 2°) Ahorar vamos a hacer un Label_Predict y le vamos a dar un PREDICT a los datos de Prueba(test) --->\n","\n","label_predict = LR.predict(matriz_test)\n","CR = classification_report(test.categoria, label_predict)\n","print(CR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8So99v1g5vco","executionInfo":{"status":"ok","timestamp":1720553360087,"user_tz":180,"elapsed":2101,"user":{"displayName":"mariana ibarra","userId":"08710446199790177814"}},"outputId":"6efe60fa-99cb-459b-b303-92cb440a855e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                      precision    recall  f1-score   support\n","\n","ciencia y tecnologia       0.63      0.56      0.59      2864\n","            deportes       0.70      0.73      0.72      4277\n","            economia       0.62      0.68      0.65      5461\n","     entretenimiento       0.58      0.60      0.59      3580\n","               mundo       0.59      0.60      0.59      2838\n","            politica       0.59      0.50      0.54      3941\n","\n","            accuracy                           0.62     22961\n","           macro avg       0.62      0.61      0.61     22961\n","        weighted avg       0.62      0.62      0.62     22961\n","\n"]}]},{"cell_type":"markdown","source":["***CONCLUSION:***\n","\n","Ahora sí obtuvimos unos resultados interesantes. Vean ustedes que en exactitud mi modelo tuvo 0.63 comparado a una exactitud de 0.24, o sea 63%, es muy buen resultado realmente, no es malo, o sea, cualquier cosa encima de 60 pues debe considerarse que no es tan malo, es así.\n","\n","En ciencia y tecnología él tuvo una buena precisión. Recuerden ustedes que la precisión entonces disminuye el error tipo 1, o sea, el error de falsos positivos, o sea, generalizó bien, contra un recall de 0.56. Y también, por ejemplo, en deportes, entonces él más o menos que precisión y recall estuvo muy equilibrado, de nuestras 4272 noticias, en economía.\n","\n","Priorizo recall, o sea hubo más error tipo 1, pero aquí mejoró el error tipo 2, digamos hubo más falsos negativos aquí, en este caso. Sin embargo, pues son valores muy cercanos, 0.61, 0.60, 0.61, 0.62. Y aquí en política fue donde no generalizó tan bien, tuvo un 59 contra un 49 de recall, un 59 contra un 49 de recall.\n","\n","Entonces en términos generales, nosotros podemos decir que nuestro modelo generalizó comparativamente mucho mejor. Vean ustedes que nuevamente la exactitud fue el del 0.63, o sea 63% de exactitud, o sea de las 22961 titulares clasificó bien el 63%. Eso es bueno, la media macro, la media general estuvo también 62% tanto en precisión como recall. Muy bueno.\n","\n","Y la media ponderada 63%. Entonces, ¿qué podemos concluir con este resultado? Que a través de una regresión logística, que es un modelo de los más robustos, porque él generaliza mejor y no es tan complejo, digamos, obtenemos resultados interesantes.\n","\n","Podemos, lógicamente, en vez de utilizar en nuestra clasificación una combinación de vectores por suma, nosotros podríamos hacer n combinación de vectores, puede ser suma, cualquier tipo de operación que nosotros queramos realizar aquí para combinar nuestros vectores para obtener un vector resultante. Ya esto ya depende de lo que queramos hacer.\n","\n","Y lo otro que también podríamos hacer es usar otros modelos, no solamente una regresión logística. Podríamos utilizar Support Vector Machine, podríamos utilizar Random Forest, podríamos utilizar árboles de decisión, podríamos utilizar todo tipo de clasificadores, bagging, stacking, en fin, todos los clasificadores que deseemos para ver qué modelo se adapta mejor.\n","\n","También podríamos por ejemplo modificar algunos hiperparámetros aquí en nuestra regresión logística, no solamente el número máximo de iteraciones, sino otros parámetros, y así entonces podemos ir haciéndole un tunning a nuestro modelo para dejarlo mejor.\n","\n","¿a costo de qué? Si nosotros nos vamos a colocar más precisos y vamos a digamos a querer que generalicen, digamos, tenga mejores resultados, podemos estar también sacrificando nuestro modelo al hacerlos tan exactos, porque al recibir nuevos datos, entonces él puede presentar más errores.\n","\n","Pero lógicamente ahí es donde entra en juegos la ciencia de datos, la función del científico de datos para ver cómo puede mejorar su modelo y que él aún continúe generalizando bien, para los datos que nunca antes vio.\n","Entonces básicamente, en esto consiste nuestro curso inicial de procesamiento del lenguaje humano, utilizando Word2Vec.\n"],"metadata":{"id":"5WG_bJCq-1OW"}},{"cell_type":"markdown","source":["**<font color=orage>CONCLUSION FINAL:</font>**\n","\n","Concluimos con éxito nuestro entrenamiento, the Word2Vec, interpretación del lenguaje humano con Word Embedding. Entonces, cómo pudiste observar a lo largo de nuestro curso pudimos explorar un dataset, dos datasets, realmente, de noticias, uno con noticias ya procesadas para el entrenamiento y otro para realizar nuestras pruebas.\n","\n","Utilizamos CountVectorizer para ver cómo funcionaba One-Hot-Encoding, creando vectores de palabras, colocando 1 cuando reconocía la palabra y 0 en los demás casos, pero que esto no era algo práctico o viable, porque si teníamos 50000 palabras, entonces digamos el vector iba a ser de una dimensión de 1 por 50000 y solamente un 1 y el resto 0 entonces esto no sería muy recomendable en términos de almacenamiento, por ejemplo.\n","\n","También aprendimos a cargar un modelo Word2Vec utilizando digamos un archivo .zip que almacenamos en nuestro Google Drive. Los cargamos con gensim models, entendimos cómo funciona Word2Vec, cómo es que se generan estos vectores de palabras.\n","\n","Recordemos que utilizamos un modelo de 400 dimensiones y pudimos sumarlas, restarlas, ver cómo es que se comportaba nuestro modelo. Bastante interesante todo este tema y aprendimos también a combinar estos vectores a través de suma. Un pre procesamiento para poder hacer tokens con estos vectores y después sumar esos tokens y combinarlos creando un solo vector para cada titular.\n","\n","Y así entonces pudimos llegar a la clasificación de nuestros textos, generando una matriz de textos utilizando NumPy, lógicamente entrenamos digamos nuestra matriz, utilizando un DummyClassifier, inicialmente para generar un baseline y después una regresión logística, que nos permitió a nosotros obtener resultados relativamente interesantes. ¿Y qué podemos concluir de nuestro curso realmente?\n","\n","Que mientras más datos haya, nuestro modelo va a generalizar mucho mejor. Por eso es tan importante que cuando nosotros vayamos a trabajar con Word Embedding tengamos muchos, muchos, muchos y muchos datos. Lo que hicimos fue con un dataset relativamente pequeño, de alrededor de 120000 noticias en total, pero si ustedes pueden trabajar con datasets más grandes, ustedes van a ver que él va a generalizar mucho mejor.\n","\n","Y lo otro es que aún hay mucho por hacer en el área de Word Embedding en el idioma castellano y esto realmente es un factor limitante en todo lo que se podría hacer para generar otro tipo de modelos, trabajando con otro tipo de arquitecturas, pero básicamente este es nuestro curso."],"metadata":{"id":"xX0S1qgv_3_C"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
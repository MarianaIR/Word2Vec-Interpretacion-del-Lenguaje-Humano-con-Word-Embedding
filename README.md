# üì∞ Clasificaci√≥n de Noticias con Word2Vec: Interpretaci√≥n del Lenguaje Humano

[![Python](https://img.shields.io/badge/Python-3670A0?style=flat&logo=python&logoColor=ffdd54)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-013243?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)

Este proyecto desarrolla un **Clasificador de Noticias** para la *start-up* **Alura Latam News** utilizando t√©cnicas de **Word Embedding** y **Procesamiento de Lenguaje Natural (NLP)**. El objetivo es transformar los titulares de noticias en vectores num√©ricos (embeddings) para automatizar su clasificaci√≥n en categor√≠as tem√°ticas.

---

## üöÄ Metodolog√≠a del Proyecto

El flujo de trabajo se centra en el uso del modelo **Word2Vec** para la vectorizaci√≥n del lenguaje, seguido por el entrenamiento de un modelo de clasificaci√≥n tradicional.

### 1. Preparaci√≥n y Carga de Datos
* **Datasets:** Se utilizan los archivos `noticias_entrenamiento.csv` y `noticias_prueba.csv`.
* **Contenido:** Los datos incluyen el `titulo` de la noticia y su `categoria` (ej.: `deportes`, `ciencia y tecnolog√≠a`, `econom√≠a`, `pol√≠tica`, etc.).
* **Vectorizaci√≥n Base:** El proceso comienza entendiendo la representaci√≥n simple como **ONE-HOT-ENCODING** antes de la aplicaci√≥n de Word Embedding.

### 2. Word Embedding (Vectorizaci√≥n Sem√°ntica)
* **Modelo Utilizado:** Se carg√≥ el modelo pre-entrenado para espa√±ol **`SBW-vectors-300-min5.bin.gz`** (ya que otra fuente result√≥ no funcional).
* **Proceso:**
    1.  **Tokenizaci√≥n** de los titulares de las noticias.
    2.  Generaci√≥n del vector de *embedding* para cada titular mediante la **suma de los vectores** de las palabras que lo componen.
    3.  Creaci√≥n de una matriz de textos utilizando **NumPy** a partir de estos vectores para el entrenamiento.

### 3. Entrenamiento y Clasificaci√≥n
* **Baseline:** Inicialmente, se entrena un **`DummyClassifier`** (provisto por **Scikit-learn**) para establecer un punto de referencia de rendimiento.
* **Modelo Final:** La clasificaci√≥n se realiza con un modelo de **Regresi√≥n Log√≠stica** (provisto por **Scikit-learn**) entrenado sobre la matriz de *embeddings* generada.

---

## üõ†Ô∏è Tecnolog√≠as Clave

| Herramienta | Uso Principal |
| :--- | :--- |
| **Python** | Lenguaje de programaci√≥n. |
| **Word2Vec** | Modelo de Word Embedding. |
| **NumPy** | Creaci√≥n y manipulaci√≥n de matrices de texto (vectores). |
| **Scikit-learn** | Provee los clasificadores **Regresi√≥n Log√≠stica** y **DummyClassifier**. |

---

## üí° Conclusiones del Aprendizaje

1.  **Necesidad de Datos Masivos:** La **generalizaci√≥n del modelo mejora significativamente con el volumen de datos**. Se resalta que el *dataset* utilizado (alrededor de **120,000 noticias**) es relativamente peque√±o y se recomienda trabajar con vol√∫menes de datos mucho mayores.
2.  **L√≠mite en Castellano:** A√∫n existe un gran potencial de desarrollo en el √°rea de Word Embedding para el **idioma castellano**, siendo la disponibilidad de modelos y recursos pre-entrenados un factor que limita el desarrollo de arquitecturas m√°s complejas.
